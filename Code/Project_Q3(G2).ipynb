{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install -y gdown","metadata":{"id":"qvnA303Bq34_","outputId":"ec2d8db0-0186-4124-b14c-dd6ab02022e4","execution":{"iopub.status.busy":"2024-02-03T23:11:53.610942Z","iopub.execute_input":"2024-02-03T23:11:53.611276Z","iopub.status.idle":"2024-02-03T23:17:41.380234Z","shell.execute_reply.started":"2024-02-03T23:11:53.611248Z","shell.execute_reply":"2024-02-03T23:17:41.379266Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): \\ WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\ndone\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 23.7.4\n  latest version: 23.11.0\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.11.0\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2024.2.2   |       hbcca054_0         152 KB  conda-forge\n    certifi-2023.11.17         |     pyhd8ed1ab_1         155 KB  conda-forge\n    filelock-3.13.1            |     pyhd8ed1ab_0          15 KB  conda-forge\n    gdown-5.1.0                |     pyhd8ed1ab_0          21 KB  conda-forge\n    libblas-3.9.0              |20_linux64_openblas          14 KB  conda-forge\n    libcblas-3.9.0             |20_linux64_openblas          14 KB  conda-forge\n    liblapack-3.9.0            |20_linux64_openblas          14 KB  conda-forge\n    libopenblas-0.3.25         |pthreads_h413a1c8_0         5.3 MB  conda-forge\n    openssl-3.2.1              |       hd590300_0         2.7 MB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         8.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.13.1-pyhd8ed1ab_0 \n  gdown              conda-forge/noarch::gdown-5.1.0-pyhd8ed1ab_0 \n\nThe following packages will be UPDATED:\n\n  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.2.2-hbcca054_0 \n  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2023.11.17-pyhd8ed1ab_1 \n  openssl                                  3.2.0-hd590300_1 --> 3.2.1-hd590300_0 \n\nThe following packages will be DOWNGRADED:\n\n  libblas                         3.9.0-21_linux64_openblas --> 3.9.0-20_linux64_openblas \n  libcblas                        3.9.0-21_linux64_openblas --> 3.9.0-20_linux64_openblas \n  liblapack                       3.9.0-21_linux64_openblas --> 3.9.0-20_linux64_openblas \n  libopenblas                    0.3.26-pthreads_h413a1c8_0 --> 0.3.25-pthreads_h413a1c8_0 \n\n\n\nDownloading and Extracting Packages\nliblapack-3.9.0      | 14 KB     |                                       |   0% \nlibblas-3.9.0        | 14 KB     |                                       |   0% \u001b[A\n\nfilelock-3.13.1      | 15 KB     |                                       |   0% \u001b[A\u001b[A\n\n\nlibcblas-3.9.0       | 14 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nlibopenblas-0.3.25   | 5.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\ngdown-5.1.0          | 21 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\ncertifi-2023.11.17   | 155 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nopenssl-3.2.1        | 2.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nca-certificates-2024 | 152 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nliblapack-3.9.0      | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\nfilelock-3.13.1      | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\nlibblas-3.9.0        | 14 KB     | ##################################### | 100% \u001b[A\n\n\nlibcblas-3.9.0       | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\n\n\ncertifi-2023.11.17   | 155 KB    | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nopenssl-3.2.1        | 2.7 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\ngdown-5.1.0          | 21 KB     | ############################2         |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nca-certificates-2024 | 152 KB    | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\nfilelock-3.13.1      | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\n\n\n\nliblapack-3.9.0      | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\nlibblas-3.9.0        | 14 KB     | ##################################### | 100% \u001b[A\n\n\n\n\n\n\nopenssl-3.2.1        | 2.7 MB    | #################3                    |  47% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nlibcblas-3.9.0       | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\nlibopenblas-0.3.25   | 5.3 MB    | ###################4                  |  53% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\ncertifi-2023.11.17   | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\ncertifi-2023.11.17   | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\ngdown-5.1.0          | 21 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nca-certificates-2024 | 152 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\nca-certificates-2024 | 152 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nopenssl-3.2.1        | 2.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\nopenssl-3.2.1        | 2.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nlibopenblas-0.3.25   | 5.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown 1vXuN6h5pMha_dmU_mWhn2e1fsCANGoOT\n!gdown 1HxoefWIplFdkKT6mPOFTO-aIrE788jRN","metadata":{"id":"ZUK2Msfkq35B","outputId":"f3715178-576f-45ae-cb6c-06f534f239d6","execution":{"iopub.status.busy":"2024-02-03T23:39:25.961968Z","iopub.execute_input":"2024-02-03T23:39:25.962757Z","iopub.status.idle":"2024-02-03T23:39:30.999881Z","shell.execute_reply.started":"2024-02-03T23:39:25.962714Z","shell.execute_reply":"2024-02-03T23:39:30.998706Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1vXuN6h5pMha_dmU_mWhn2e1fsCANGoOT\nFrom (redirected): https://drive.google.com/uc?id=1vXuN6h5pMha_dmU_mWhn2e1fsCANGoOT&confirm=t&uuid=bf82d1ed-dc3d-4c2e-91e7-3f130ecc71bd\nTo: /kaggle/working/subtaskB_train.jsonl\n100%|█████████████████████████████████████████| 155M/155M [00:00<00:00, 204MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1HxoefWIplFdkKT6mPOFTO-aIrE788jRN\nTo: /kaggle/working/subtaskB_dev.jsonl\n100%|███████████████████████████████████████| 4.93M/4.93M [00:00<00:00, 254MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Required Packages\nimport torch.nn as nn\nimport torch\nimport random\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer\nimport json\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AutoConfig\nimport time\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"id":"Eab_8Zd9q35C","execution":{"iopub.status.busy":"2024-02-03T23:39:31.001910Z","iopub.execute_input":"2024-02-03T23:39:31.002228Z","iopub.status.idle":"2024-02-03T23:39:38.678148Z","shell.execute_reply.started":"2024-02-03T23:39:31.002200Z","shell.execute_reply":"2024-02-03T23:39:38.677183Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n#   Generator 1\n#------------------------------\nclass Generator1(nn.Module):\n    def __init__(self, noise_size=100, output_size=768, hidden_size=768, dropout_rate=0.8):\n        super(Generator1, self).__init__()\n\n        # Build layers sequentially\n        layers = []\n\n        # Linear transformation from noise to hidden_size\n        layers.append(nn.Linear(noise_size, hidden_size))\n        # Leaky ReLU activation\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        # Dropout layer\n        layers.append(nn.Dropout(dropout_rate))\n\n        # Output layer\n        layers.append(nn.Linear(hidden_size, output_size))\n\n        # Define the sequential model\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, noise):\n        # Forward pass through the layers\n        output = self.layers(noise)\n        return output","metadata":{"id":"OsmIpGn4q35C","execution":{"iopub.status.busy":"2024-02-03T23:35:04.655102Z","iopub.execute_input":"2024-02-03T23:35:04.655475Z","iopub.status.idle":"2024-02-03T23:35:04.662864Z","shell.execute_reply.started":"2024-02-03T23:35:04.655451Z","shell.execute_reply":"2024-02-03T23:35:04.661796Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n#   Generator 2\n#------------------------------\nclass Generator2(nn.Module):\n    def __init__(self, bert_model, output_size=768):\n        super(Generator2, self).__init__()\n        self.bert = bert_model\n        self.output_size = output_size\n\n    def forward(self, bag_of_words):\n        bag_of_words = bag_of_words.to(device)\n        sampled_indices = torch.randint(0, len(bag_of_words), (100,))\n        sampled_words = [str(bag_of_words[idx]) for idx in sampled_indices]\n\n        # Convert the sampled words to a tensor of token IDs using BERT tokenizer\n        tokenized_input = tokenizer(sampled_words, return_tensors='pt', padding=True, truncation=True).to(device)\n\n        # Forward pass through the BERT model\n        outputs = self.bert(**tokenized_input.to(device))\n\n        # Extract the output embeddings from the BERT model\n        embeddings = (outputs.last_hidden_state).to(device)\n\n        # Average pooling over the embeddings\n        avg_pooled = torch.mean(embeddings, dim=1).to(device)\n\n        return avg_pooled","metadata":{"id":"qIGOjRIxq35C","execution":{"iopub.status.busy":"2024-02-03T23:39:38.679936Z","iopub.execute_input":"2024-02-03T23:39:38.680412Z","iopub.status.idle":"2024-02-03T23:39:38.688695Z","shell.execute_reply.started":"2024-02-03T23:39:38.680384Z","shell.execute_reply":"2024-02-03T23:39:38.687743Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n#   The Discriminator\n#------------------------------\nclass Discriminator(nn.Module):\n    def __init__(self, input_size=768, hidden_size=768, num_labels=6, dropout_rate=0.8):\n        super(Discriminator, self).__init__()\n\n        # Input dropout\n        self.input_dropout = nn.Dropout(p=dropout_rate)\n\n        # Feature extraction layer\n        self.feature_layer = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(dropout_rate)\n        )\n\n        # Output layer for classification\n        self.logit = nn.Linear(hidden_size, num_labels + 1) # +1 for the probability of this sample being fake/real.\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, input):\n        # Apply input dropout\n        input = self.input_dropout(input)\n\n        # Forward pass through feature extraction layers\n        feature = self.feature_layer(input)\n\n        # Output layer\n        logits = self.logit(feature)\n        probs = self.softmax(logits)\n\n        return feature, logits, probs","metadata":{"id":"QxKi-_nbq35D","execution":{"iopub.status.busy":"2024-02-03T23:39:38.690261Z","iopub.execute_input":"2024-02-03T23:39:38.690624Z","iopub.status.idle":"2024-02-03T23:39:38.703616Z","shell.execute_reply.started":"2024-02-03T23:39:38.690587Z","shell.execute_reply":"2024-02-03T23:39:38.702761Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"##Set random values\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\nif torch.cuda.is_available():\n  torch.cuda.manual_seed_all(seed_val)","metadata":{"id":"y_7x5Vxkq35D","execution":{"iopub.status.busy":"2024-02-03T23:39:58.073058Z","iopub.execute_input":"2024-02-03T23:39:58.073825Z","iopub.status.idle":"2024-02-03T23:39:58.080394Z","shell.execute_reply.started":"2024-02-03T23:39:58.073794Z","shell.execute_reply":"2024-02-03T23:39:58.079214Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Check if CUDA (GPU) is available, and assign the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Pre-trained BERT model name\nmodel_name = \"bert-base-cased\"\n\n# Load the pre-trained BERT model\ntransformer = AutoModel.from_pretrained(model_name)\n\n# Load the corresponding tokenizer for BERT\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GEFp1UZq35D","outputId":"b664f7f0-9e82-4077-9499-a883d8c9b695","execution":{"iopub.status.busy":"2024-02-03T23:40:08.395270Z","iopub.execute_input":"2024-02-03T23:40:08.396147Z","iopub.status.idle":"2024-02-03T23:40:08.729224Z","shell.execute_reply.started":"2024-02-03T23:40:08.396107Z","shell.execute_reply":"2024-02-03T23:40:08.728208Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def parse_question_classification_file(input_file):\n    examples = []\n\n    with open(input_file, 'r') as f:\n        # Iterate through each line in the JSONL file\n        for line in f:\n            # Parse the JSON from each line\n            data = json.loads(line)\n\n            # Extract relevant information\n            text = data.get('text', '')\n            label = data.get('label', '')\n            model = data.get('model', '')\n\n            examples.append((text, label, model))\n\n    return examples\n\n\n\ndef generate_data_loader(examples, label_masks, label_map, max_seq_length=32, batch_size=1, do_shuffle=False):\n    new_example = []\n    for index, ex in enumerate(examples):\n      new_example.append((ex, label_masks[index]))\n\n    input_ids = []\n    input_mask_array = []\n    label_mask_array = []\n    label_id_array = []\n\n    # Tokenization and Bag of Words\n    bag_of_words_vectorizer = CountVectorizer()\n    bag_of_words_matrix = bag_of_words_vectorizer.fit_transform([ex[0] for ex, _ in new_example])\n\n    # Tokenization\n    for (ex, label_mask) in new_example:\n        # Assuming tokenizer is predefined\n        if ex[0] == '':\n          print(\"qq\")\n        encoded_sent = tokenizer.encode(ex[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n        input_ids.append(encoded_sent)\n        label_id_array.append(label_map.get(ex[2]))\n        label_mask_array.append(label_mask)\n\n    # Attention mask\n    input_mask_array = [[int(token_id > 0) for token_id in sent] for sent in input_ids]\n\n    # Convert to tensors\n    input_ids = torch.tensor(input_ids)\n    input_mask_array = torch.tensor(input_mask_array)\n    label_id_array = torch.tensor(label_id_array)\n    label_mask_array = torch.tensor(label_mask_array)\n\n    # Building the TensorDataset\n    dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n\n    # Choose sampler based on shuffle option\n    sampler = RandomSampler if do_shuffle else SequentialSampler\n\n    # Building the DataLoader\n    return DataLoader(\n        dataset,\n        sampler=sampler(dataset),\n        batch_size=batch_size\n    ), bag_of_words_matrix\n\n\n\n\ndef load_and_generate_data_loader(file_path, label_map, percentage_labeled, do_shuffle=False):\n    # Load the examples\n    examples = parse_question_classification_file(file_path)\n\n    # Calculate the number of examples to be labeled\n    num_labeled_examples = int(len(examples) * (percentage_labeled / 100))\n\n    # Split the examples into labeled and unlabeled\n    labeled_examples = examples[:num_labeled_examples]\n    unlabeled_examples = examples[num_labeled_examples:]\n\n    # The labeled dataset is assigned with a mask set to True\n    label_masks = np.ones(len(labeled_examples), dtype=bool)\n\n    # If unlabeled examples are available\n    if unlabeled_examples:\n        labeled_examples = labeled_examples + unlabeled_examples\n        # The unlabeled dataset is assigned with a mask set to False\n        tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n        label_masks = np.concatenate([label_masks, tmp_masks])\n\n    # Generate DataLoaders\n    dataloader, bag_of_words_matrix = generate_data_loader(labeled_examples, label_masks, label_map, do_shuffle=do_shuffle)\n\n    return dataloader, bag_of_words_matrix","metadata":{"id":"sBXW8sRmq35E","execution":{"iopub.status.busy":"2024-02-03T23:40:11.878403Z","iopub.execute_input":"2024-02-03T23:40:11.879148Z","iopub.status.idle":"2024-02-03T23:40:11.895039Z","shell.execute_reply.started":"2024-02-03T23:40:11.879112Z","shell.execute_reply":"2024-02-03T23:40:11.893784Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define the file paths\ntrain_file_path = \"subtaskB_train.jsonl\"\ntest_file_path = \"subtaskB_dev.jsonl\"\n\n# Assuming label_map is predefined\nlabel_map = {\"human\": 0, \"chatGPT\": 1, \"cohere\": 2, \"davinci\": 3, \"bloomz\": 4, \"dolly\": 5}\n\n# Generate DataLoaders for training and testing for each percentage\n\ntrain_dataloader_50, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 50, do_shuffle=True)\ntest_dataloader, _ = load_and_generate_data_loader(test_file_path, label_map, 100, do_shuffle=False)","metadata":{"execution":{"iopub.execute_input":"2024-02-03T17:14:30.845344Z","iopub.status.busy":"2024-02-03T17:14:30.844996Z","iopub.status.idle":"2024-02-03T17:16:42.962259Z","shell.execute_reply":"2024-02-03T17:16:42.961461Z","shell.execute_reply.started":"2024-02-03T17:14:30.845320Z"},"id":"sfwrpWQzq35E","trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Instantiate the Generator and Discriminator\ngenerator1_1 = Generator1()\ngenerator1_5 = Generator1()\ngenerator1_10 = Generator1()\ngenerator1_50 = Generator1()\ndiscriminator_1 = Discriminator()\ndiscriminator_5 = Discriminator()\ndiscriminator_10 = Discriminator()\ndiscriminator_50 = Discriminator()\n\n# Put everything on the GPU if available\ngenerator1_1.to(device)\ngenerator1_5.to(device)\ngenerator1_10.to(device)\ngenerator1_50.to(device)\ndiscriminator_1.to(device)\ndiscriminator_5.to(device)\ndiscriminator_10.to(device)\ndiscriminator_50.to(device)\ntransformer.to(device)\n\nmulti_gpu = False\n\n# Use DataParallel if multi_gpu is True\nif multi_gpu and torch.cuda.is_available():\n    generator1_1 = torch.nn.DataParallel(generator1_1)\n    generator1_5 = torch.nn.DataParallel(generator1_5)\n    generator1_10 = torch.nn.DataParallel(generator1_10)\n    generator1_50 = torch.nn.DataParallel(generator1_50)\n    discriminator_1 = torch.nn.DataParallel(discriminator_1)\n    discriminator_5 = torch.nn.DataParallel(discriminator_5)\n    discriminator_10 = torch.nn.DataParallel(discriminator_10)\n    discriminator_50 = torch.nn.DataParallel(discriminator_50)","metadata":{"execution":{"iopub.execute_input":"2024-02-03T17:16:42.965024Z","iopub.status.busy":"2024-02-03T17:16:42.964731Z","iopub.status.idle":"2024-02-03T17:16:43.030548Z","shell.execute_reply":"2024-02-03T17:16:43.029725Z","shell.execute_reply.started":"2024-02-03T17:16:42.965000Z"},"id":"QabNtbNKq35E","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Hyperparameters\nlearning_rate_discriminator = 5e-5\nlearning_rate_generator = 7e-4\nepsilon = 1e-8\nnum_train_epochs = 5\nprint_each_n_step = 100\n\n# Extract model parameters for Discriminator, Generator, and Transformer\ntransformer_vars = [i for i in transformer.parameters()]\nd_vars_1 = [v for v in discriminator_1.parameters()]\nd_vars_5 = [v for v in discriminator_5.parameters()]\nd_vars_10 = [v for v in discriminator_10.parameters()]\nd_vars_50 = [v for v in discriminator_50.parameters()]\ng_vars_1 = [v for v in generator1_1.parameters()]\ng_vars_5 = [v for v in generator1_5.parameters()]\ng_vars_10 = [v for v in generator1_10.parameters()]\ng_vars_50 = [v for v in generator1_50.parameters()]\n\n# Set up optimizers for Discriminator and Generator\ndis_optimizer_1 = torch.optim.AdamW(d_vars_1, lr=learning_rate_generator)\ndis_optimizer_5 = torch.optim.AdamW(d_vars_5, lr=learning_rate_generator)\ndis_optimizer_10 = torch.optim.AdamW(d_vars_10, lr=learning_rate_generator)\ndis_optimizer_50 = torch.optim.AdamW(d_vars_50, lr=learning_rate_generator)\ngen_optimizer_1 = torch.optim.AdamW(g_vars_1, lr=learning_rate_generator)\ngen_optimizer_5 = torch.optim.AdamW(g_vars_5, lr=learning_rate_generator)\ngen_optimizer_10 = torch.optim.AdamW(g_vars_10, lr=learning_rate_generator)\ngen_optimizer_50 = torch.optim.AdamW(g_vars_50, lr=learning_rate_generator)\n\n\n\n\nbert_optimizer_1 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_5 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_10 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_50 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"execution":{"iopub.execute_input":"2024-02-03T17:16:43.031959Z","iopub.status.busy":"2024-02-03T17:16:43.031657Z","iopub.status.idle":"2024-02-03T17:16:43.047560Z","shell.execute_reply":"2024-02-03T17:16:43.046814Z","shell.execute_reply.started":"2024-02-03T17:16:43.031935Z"},"id":"0QIE0KQAq35F","outputId":"4b72e823-3e3f-4912-e0b3-8b5866b2b7e7","trusted":true},"execution_count":14,"outputs":[{"ename":"NameError","evalue":"name 'generator1_1' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-d01cab0b4343>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0md_vars_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdiscriminator_10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0md_vars_50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdiscriminator_50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mg_vars_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator1_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mg_vars_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator1_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mg_vars_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator1_10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'generator1_1' is not defined"]}]},{"cell_type":"markdown","source":"## Train with 50 % labeled data","metadata":{"id":"7fSt0gDRq35F"}},{"cell_type":"code","source":"test_accuracies = []\nf1_scores = []\n\n\ndef train(transformer, generator1, discriminator, train_dataloader,test_dataloader, gen_optimizer, dis_optimizer, bert_optimizer, device, num_train_epochs, print_each_n_step=100, epsilon=1e-12):\n\n    test_accuracy = 0\n    f1 = 0\n\n    for epoch_i in range(num_train_epochs):\n        print(f\"\\n======== Epoch {epoch_i + 1} / {num_train_epochs} ========\")\n\n        tr_g_loss = 0\n        tr_d_loss = 0\n\n        # Set models to training mode\n        transformer.train()\n        generator1.train()\n        discriminator.train()\n\n        # Iterate through batches in the training dataloader\n        for step, batch in enumerate(train_dataloader):\n            # Display progress every print_each_n_step batches\n            if step % print_each_n_step == 0 and not step == 0:\n                print(f\"  Batch {step:>5,}  of  {len(train_dataloader):>5,}.\")\n\n            # Move batch tensors to device\n            b_input_ids, b_input_mask, b_labels, b_label_mask = [tensor.to(device) for tensor in batch]\n            real_batch_size = b_input_ids.shape[0]\n\n            # Forward pass through the transformer\n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n\n            # Generate fake data using the generator\n            noise = torch.zeros(real_batch_size, 100, device=device).uniform_(0, 1)\n            gen_rep = generator1(noise)\n\n            # Concatenate real and fake data for the discriminator input\n            discriminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n            features, logits, probs = discriminator(discriminator_input)\n\n            # Split features, logits, and probs for real and fake data\n            features_list = torch.split(features, real_batch_size)\n            D_real_features, D_fake_features = features_list[0], features_list[1]\n\n            logits_list = torch.split(logits, real_batch_size)\n            D_real_logits, D_fake_logits = logits_list[0], logits_list[1]\n\n            probs_list = torch.split(probs, real_batch_size)\n            D_real_probs, D_fake_probs = probs_list[0], probs_list[1]\n\n            # Generator's loss calculation\n            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:, -1] + epsilon))\n            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n            g_loss = g_loss_d + g_feat_reg\n\n            # Discriminator's loss calculation\n            logits = D_real_logits[:, 0:-1]\n            log_probs = F.log_softmax(logits, dim=-1)\n            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_map))\n            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n            labeled_example_count = per_example_loss.type(torch.float32).numel()\n\n            D_L_Supervised = 0 if labeled_example_count == 0 else torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n\n            # Backward pass and optimization\n            gen_optimizer.zero_grad()\n            dis_optimizer.zero_grad()\n            bert_optimizer.zero_grad()\n            g_loss.backward(retain_graph=True)\n            d_loss.backward()\n            gen_optimizer.step()\n            dis_optimizer.step()\n            bert_optimizer.step()\n\n\n            # Update loss accumulators\n            tr_g_loss += g_loss.item()\n            tr_d_loss += d_loss.item()\n\n        # Calculate average training losses\n        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n        avg_train_loss_d = tr_d_loss / len(train_dataloader)\n\n        # Print average training losses for the epoch\n        print(\"\\n  Average training loss generator: {:.3f}\".format(avg_train_loss_g))\n        print(\"  Average training loss discriminator: {:.3f} \\n\\n\".format(avg_train_loss_d))\n        test_accuracy, avg_test_loss, f1 = test(transformer, discriminator, test_dataloader, device)\n        print(\"\\n  Accuracy on Test Set: {:.3f}\".format(test_accuracy))\n        print(\"  Average Test Loss: {:.3f}\".format(avg_test_loss))\n        print(\"  F1 Score on Test Set: {:.3f}\".format(f1))\n\n\n    return test_accuracy, f1\n\n\ndef test(transformer, discriminator, test_dataloader, device):\n\n\n    # Set models to evaluation mode\n    transformer.eval()\n    discriminator.eval()\n\n    # Initialize variables for test evaluation\n    total_test_accuracy = 0\n    total_test_loss = 0\n    nb_test_steps = 0\n\n    all_preds = []\n    all_labels_ids = []\n\n    # Define the loss function for evaluation\n    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n\n    # Iterate through the test dataloader\n    for batch in test_dataloader:\n        # Move batch tensors to the device (GPU or CPU)\n        b_input_ids, b_input_mask, b_labels, _ = [tensor.to(device) for tensor in batch]\n\n        # Perform forward pass without gradient computation\n        with torch.no_grad():\n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n            _, logits, probs = discriminator(hidden_states)\n\n            # Extract logits for labeled classes (excluding the fake class)\n            filtered_logits = logits[:, 0:-1]\n            total_test_loss += nll_loss(filtered_logits, b_labels)\n\n        # Calculate accuracy and accumulate predictions and labels\n        _, preds = torch.max(filtered_logits, 1)\n        all_preds += preds.detach().cpu()\n        all_labels_ids += b_labels.detach().cpu()\n\n    # Convert accumulated predictions and labels to NumPy arrays\n    all_preds = torch.stack(all_preds).numpy()\n    all_labels_ids = torch.stack(all_labels_ids).numpy()\n\n    # Calculate accuracy and average test loss\n    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n    avg_test_loss = total_test_loss / len(test_dataloader)\n    avg_test_loss = avg_test_loss.item()\n\n    # Calculate F1 score\n    f1 = f1_score(all_labels_ids, all_preds, average='weighted')\n\n    return test_accuracy, avg_test_loss, f1\n\n\n","metadata":{"execution":{"iopub.execute_input":"2024-02-03T17:16:43.049675Z","iopub.status.busy":"2024-02-03T17:16:43.049406Z","iopub.status.idle":"2024-02-03T17:16:43.077823Z","shell.execute_reply":"2024-02-03T17:16:43.077079Z","shell.execute_reply.started":"2024-02-03T17:16:43.049654Z"},"id":"NgMDslDnq35G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_acc, f1 = train(transformer, generator1_50, discriminator_50, train_dataloader_50,test_dataloader, gen_optimizer_50, dis_optimizer_50, bert_optimizer_50,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1)\ntest_accuracies.append(test_acc)","metadata":{"execution":{"iopub.execute_input":"2024-02-03T17:16:43.079677Z","iopub.status.busy":"2024-02-03T17:16:43.079017Z"},"id":"xELUkmPuq35G","outputId":"2961471d-a653-432c-d405-154a08173960","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\n======== Epoch 1 / 5 ========\n\n  Batch   100  of  2,220.\n\n  Batch   200  of  2,220.\n\n  Batch   300  of  2,220.\n\n  Batch   400  of  2,220.\n\n  Batch   500  of  2,220.\n\n  Batch   600  of  2,220.\n\n  Batch   700  of  2,220.\n\n  Batch   800  of  2,220.\n\n  Batch   900  of  2,220.\n\n  Batch 1,000  of  2,220.\n\n  Batch 1,100  of  2,220.\n\n  Batch 1,200  of  2,220.\n\n  Batch 1,300  of  2,220.\n\n  Batch 1,400  of  2,220.\n\n  Batch 1,500  of  2,220.\n\n  Batch 1,600  of  2,220.\n\n  Batch 1,700  of  2,220.\n\n  Batch 1,800  of  2,220.\n\n  Batch 1,900  of  2,220.\n\n  Batch 2,000  of  2,220.\n\n  Batch 2,100  of  2,220.\n\n  Batch 2,200  of  2,220.\n\n\n\n  Average training loss generator: 0.740\n\n  Average training loss discriminator: 1.618 \n\n\n\n\n\n\n\n  Accuracy on Test Set: 0.478\n\n  Average Test Loss: 1.951\n\n  F1 Score on Test Set: 0.415\n\n\n\n======== Epoch 2 / 5 ========\n\n  Batch   100  of  2,220.\n\n  Batch   200  of  2,220.\n\n  Batch   300  of  2,220.\n\n  Batch   400  of  2,220.\n\n  Batch   500  of  2,220.\n\n  Batch   600  of  2,220.\n\n  Batch   700  of  2,220.\n\n  Batch   800  of  2,220.\n\n  Batch   900  of  2,220.\n\n  Batch 1,000  of  2,220.\n\n  Batch 1,100  of  2,220.\n\n  Batch 1,200  of  2,220.\n\n  Batch 1,300  of  2,220.\n\n  Batch 1,400  of  2,220.\n\n  Batch 1,500  of  2,220.\n\n  Batch 1,600  of  2,220.\n\n  Batch 1,700  of  2,220.\n\n  Batch 1,800  of  2,220.\n\n  Batch 1,900  of  2,220.\n\n  Batch 2,000  of  2,220.\n\n  Batch 2,100  of  2,220.\n\n  Batch 2,200  of  2,220.\n\n\n\n  Average training loss generator: 0.733\n\n  Average training loss discriminator: 1.200 \n\n\n\n\n\n\n\n  Accuracy on Test Set: 0.403\n\n  Average Test Loss: 2.229\n\n  F1 Score on Test Set: 0.368\n\n\n\n======== Epoch 3 / 5 ========\n\n  Batch   100  of  2,220.\n\n  Batch   200  of  2,220.\n\n  Batch   300  of  2,220.\n\n  Batch   400  of  2,220.\n\n  Batch   500  of  2,220.\n\n  Batch   600  of  2,220.\n\n  Batch   700  of  2,220.\n\n  Batch   800  of  2,220.\n\n  Batch   900  of  2,220.\n\n  Batch 1,000  of  2,220.\n\n  Batch 1,100  of  2,220.\n\n  Batch 1,200  of  2,220.\n\n  Batch 1,300  of  2,220.\n\n  Batch 1,400  of  2,220.\n\n  Batch 1,500  of  2,220.\n\n  Batch 1,600  of  2,220.\n\n  Batch 1,700  of  2,220.\n\n  Batch 1,800  of  2,220.\n\n  Batch 1,900  of  2,220.\n\n  Batch 2,000  of  2,220.\n\n  Batch 2,100  of  2,220.\n\n  Batch 2,200  of  2,220.\n\n\n\n  Average training loss generator: 0.732\n\n  Average training loss discriminator: 1.093 \n\n\n\n\n\n\n\n  Accuracy on Test Set: 0.471\n\n  Average Test Loss: 2.113\n\n  F1 Score on Test Set: 0.428\n\n\n\n======== Epoch 4 / 5 ========\n\n  Batch   100  of  2,220.\n\n  Batch   200  of  2,220.\n\n  Batch   300  of  2,220.\n\n  Batch   400  of  2,220.\n\n  Batch   500  of  2,220.\n\n  Batch   600  of  2,220.\n\n  Batch   700  of  2,220.\n\n  Batch   800  of  2,220.\n\n  Batch   900  of  2,220.\n\n  Batch 1,000  of  2,220.\n\n  Batch 1,100  of  2,220.\n\n  Batch 1,200  of  2,220.\n\n  Batch 1,300  of  2,220.\n\n  Batch 1,400  of  2,220.\n"}]},{"cell_type":"markdown","source":"## Train with 10 % labeled data","metadata":{"id":"zPvqcsk8q35G"}},{"cell_type":"code","source":"train_dataloader_10, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 10, do_shuffle=True)\nnum_train_epochs = 3\ntest_acc2, f1_2 = train(transformer, generator1_10, discriminator_10, train_dataloader_10,test_dataloader, gen_optimizer_10, dis_optimizer_10, bert_optimizer_10,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_2)\ntest_accuracies.append(test_acc2)","metadata":{"id":"UJRcvXR9q35H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train with 5 % labeled data","metadata":{"id":"-rbHSvv2q35H"}},{"cell_type":"code","source":"train_dataloader_5, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 5, do_shuffle=True)\nnum_train_epochs = 3\ntest_acc3, f1_3 = train(transformer, generator1_5, discriminator_5, train_dataloader_5,test_dataloader, gen_optimizer_5, dis_optimizer_5, bert_optimizer_5,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_3)\ntest_accuracies.append(test_acc3)","metadata":{"id":"HytzsJsSq35H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train with 1 % labeled data","metadata":{"id":"U86VqTHPq35H"}},{"cell_type":"code","source":"train_dataloader_1, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 1, do_shuffle=True)\nnum_train_epochs = 3\ntest_acc4, f1_4 = train(transformer, generator1_5, discriminator_5, train_dataloader_5,test_dataloader, gen_optimizer_5, dis_optimizer_5, bert_optimizer_5,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_4)\ntest_accuracies.append(test_acc4)","metadata":{"execution":{"iopub.execute_input":"2024-02-03T11:28:31.518478Z","iopub.status.busy":"2024-02-03T11:28:31.517687Z","iopub.status.idle":"2024-02-03T11:28:36.691968Z","shell.execute_reply":"2024-02-03T11:28:36.690979Z","shell.execute_reply.started":"2024-02-03T11:28:31.518439Z"},"id":"BRRg80ADq35H","outputId":"4dcda0d6-7d06-451c-9134-a992e2c518be","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\n  Accuracy on Test Set: 0.167\n\n  Average Test Loss: 2.680\n\n  F1 Score on Test Set: 0.048\n"}]},{"cell_type":"markdown","source":"# G2","metadata":{"id":"B1ddcNqusbAB"}},{"cell_type":"code","source":"# Define the file paths\ntrain_file_path = \"subtaskB_train.jsonl\"\ntest_file_path = \"subtaskB_dev.jsonl\"\n\n# Assuming label_map is predefined\nlabel_map = {\"human\": 0, \"chatGPT\": 1, \"cohere\": 2, \"davinci\": 3, \"bloomz\": 4, \"dolly\": 5}\ntest_dataloader, _ = load_and_generate_data_loader(test_file_path, label_map, 100, do_shuffle=False)\n\n# Instantiate the Generator and Discriminator\ngenerator2_1 = Generator2(bert_model=transformer)\ngenerator2_5 = Generator2(bert_model=transformer)\ngenerator2_10 = Generator2(bert_model=transformer)\ngenerator2_50 = Generator2(bert_model=transformer)\ndiscriminator_1 = Discriminator()\ndiscriminator_5 = Discriminator()\ndiscriminator_10 = Discriminator()\ndiscriminator_50 = Discriminator()\n\n# Put everything on the GPU if available\ngenerator2_1.to(device)\ngenerator2_5.to(device)\ngenerator2_10.to(device)\ngenerator2_50.to(device)\ndiscriminator_1.to(device)\ndiscriminator_5.to(device)\ndiscriminator_10.to(device)\ndiscriminator_50.to(device)\ntransformer.to(device)\n\nmulti_gpu = False\n\n# Use DataParallel if multi_gpu is True\nif multi_gpu and torch.cuda.is_available():\n    generator2_1 = torch.nn.DataParallel(generator2_1)\n    generator2_5 = torch.nn.DataParallel(generator2_5)\n    generator2_10 = torch.nn.DataParallel(generator2_10)\n    generator2_50 = torch.nn.DataParallel(generator2_50)\n    discriminator_1 = torch.nn.DataParallel(discriminator_1)\n    discriminator_5 = torch.nn.DataParallel(discriminator_5)\n    discriminator_10 = torch.nn.DataParallel(discriminator_10)\n    discriminator_50 = torch.nn.DataParallel(discriminator_50)","metadata":{"id":"fXSEZf7csboD","execution":{"iopub.status.busy":"2024-02-03T23:40:23.234692Z","iopub.execute_input":"2024-02-03T23:40:23.235428Z","iopub.status.idle":"2024-02-03T23:40:27.670075Z","shell.execute_reply.started":"2024-02-03T23:40:23.235394Z","shell.execute_reply":"2024-02-03T23:40:27.669251Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_5234/1107263763.py:52: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  label_mask_array = torch.tensor(label_mask_array)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set Hyperparameters\nlearning_rate_discriminator = 5e-5\nlearning_rate_generator = 7e-4\nepsilon = 1e-8\nnum_train_epochs = 5\nprint_each_n_step = 100\n\n# Extract model parameters for Discriminator, Generator, and Transformer\ntransformer_vars = [i for i in transformer.parameters()]\nd_vars_1 = transformer_vars + [v for v in discriminator_1.parameters()]\nd_vars_5 = transformer_vars + [v for v in discriminator_5.parameters()]\nd_vars_10 = transformer_vars + [v for v in discriminator_10.parameters()]\nd_vars_50 = transformer_vars + [v for v in discriminator_50.parameters()]\ng_vars_1 = [v for v in generator2_1.parameters()]\ng_vars_5 = [v for v in generator2_5.parameters()]\ng_vars_10 = [v for v in generator2_10.parameters()]\ng_vars_50 = [v for v in generator2_50.parameters()]\n\n# Set up optimizers for Discriminator and Generator\ndis_optimizer_1 = torch.optim.AdamW(d_vars_1, lr=learning_rate_discriminator)\ndis_optimizer_5 = torch.optim.AdamW(d_vars_5, lr=learning_rate_discriminator)\ndis_optimizer_10 = torch.optim.AdamW(d_vars_10, lr=learning_rate_discriminator)\ndis_optimizer_50 = torch.optim.AdamW(d_vars_50, lr=learning_rate_discriminator)\ngen_optimizer_1 = torch.optim.AdamW(g_vars_1, lr=learning_rate_generator)\ngen_optimizer_5 = torch.optim.AdamW(g_vars_5, lr=learning_rate_generator)\ngen_optimizer_10 = torch.optim.AdamW(g_vars_10, lr=learning_rate_generator)\ngen_optimizer_50 = torch.optim.AdamW(g_vars_50, lr=learning_rate_generator)\n\nbert_optimizer_1 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_5 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_10 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_50 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)","metadata":{"id":"AQE1tEpSsmWp","execution":{"iopub.status.busy":"2024-02-03T23:40:29.152493Z","iopub.execute_input":"2024-02-03T23:40:29.153434Z","iopub.status.idle":"2024-02-03T23:40:29.172930Z","shell.execute_reply.started":"2024-02-03T23:40:29.153380Z","shell.execute_reply":"2024-02-03T23:40:29.172042Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_accuracies = []\nf1_scores = []\n\n\ndef train_2(bag_of_words_matrix, transformer, generator2, discriminator, train_dataloader,test_dataloader, gen_optimizer, dis_optimizer, bert_optimizer, device, num_train_epochs, print_each_n_step=100, epsilon=1e-12):\n\n    test_accuracy = 0\n    f1 = 0\n\n    for epoch_i in range(num_train_epochs):\n        print(f\"\\n======== Epoch {epoch_i + 1} / {num_train_epochs} ========\")\n\n        tr_g_loss = 0\n        tr_d_loss = 0\n\n        # Set models to training mode\n        transformer.train()\n        generator2.train()\n        discriminator.train()\n\n        # Iterate through batches in the training dataloader\n        for step, batch in enumerate(train_dataloader):\n            # Display progress every print_each_n_step batches\n            if step % print_each_n_step == 0 and not step == 0:\n                print(f\"  Batch {step:>5,}  of  {len(train_dataloader):>5,}.\")\n\n            # Move batch tensors to device\n            b_input_ids, b_input_mask, b_labels, b_label_mask = [tensor.to(device) for tensor in batch]\n            real_batch_size = b_input_ids.shape[0]\n\n            # Forward pass through the transformer\n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n\n            # Generate fake data using the generator\n            bag_of_words_dense = torch.tensor(bag_of_words_matrix.toarray(), device=device)\n            gen_rep = generator2(bag_of_words_dense)\n\n            # Concatenate real and fake data for the discriminator input\n            discriminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n            features, logits, probs = discriminator(discriminator_input)\n\n            # Split features, logits, and probs for real and fake data\n            features_list = torch.split(features, real_batch_size)\n            D_real_features, D_fake_features = features_list[0], features_list[1]\n\n            logits_list = torch.split(logits, real_batch_size)\n            D_real_logits, D_fake_logits = logits_list[0], logits_list[1]\n\n            probs_list = torch.split(probs, real_batch_size)\n            D_real_probs, D_fake_probs = probs_list[0], probs_list[1]\n\n            # Generator's loss calculation\n            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:, -1] + epsilon))\n            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n            g_loss = g_loss_d + g_feat_reg\n\n            # Discriminator's loss calculation\n            logits = D_real_logits[:, 0:-1]\n            log_probs = F.log_softmax(logits, dim=-1)\n            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_map))\n            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n            labeled_example_count = per_example_loss.type(torch.float32).numel()\n\n            D_L_Supervised = 0 if labeled_example_count == 0 else torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n\n            # Backward pass and optimization\n            gen_optimizer.zero_grad()\n            dis_optimizer.zero_grad()\n            bert_optimizer.zero_grad()\n            g_loss.backward(retain_graph=True)\n            d_loss.backward()\n            gen_optimizer.step()\n            dis_optimizer.step()\n            bert_optimizer.step()\n\n\n            # Update loss accumulators\n            tr_g_loss += g_loss.item()\n            tr_d_loss += d_loss.item()\n\n        # Calculate average training losses\n        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n        avg_train_loss_d = tr_d_loss / len(train_dataloader)\n\n        # Print average training losses for the epoch\n        print(\"\\n  Average training loss generator: {:.3f}\".format(avg_train_loss_g))\n        print(\"  Average training loss discriminator: {:.3f} \\n\\n\".format(avg_train_loss_d))\n        test_accuracy, avg_test_loss, f1 = test_2(transformer, discriminator, test_dataloader, device)\n        print(\"\\n  Accuracy on Test Set: {:.3f}\".format(test_accuracy))\n        print(\"  Average Test Loss: {:.3f}\".format(avg_test_loss))\n        print(\"  F1 Score on Test Set: {:.3f}\".format(f1))\n\n\n    return test_accuracy, f1\n\n\ndef test_2(transformer, discriminator, test_dataloader, device):\n\n\n    # Set models to evaluation mode\n    transformer.eval()\n    discriminator.eval()\n\n    # Initialize variables for test evaluation\n    total_test_accuracy = 0\n    total_test_loss = 0\n    nb_test_steps = 0\n\n    all_preds = []\n    all_labels_ids = []\n\n    # Define the loss function for evaluation\n    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n\n    # Iterate through the test dataloader\n    for batch in test_dataloader:\n        # Move batch tensors to the device (GPU or CPU)\n        b_input_ids, b_input_mask, b_labels, _ = [tensor.to(device) for tensor in batch]\n\n        # Perform forward pass without gradient computation\n        with torch.no_grad():\n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n            _, logits, probs = discriminator(hidden_states)\n\n            # Extract logits for labeled classes (excluding the fake class)\n            filtered_logits = logits[:, 0:-1]\n            total_test_loss += nll_loss(filtered_logits, b_labels)\n\n        # Calculate accuracy and accumulate predictions and labels\n        _, preds = torch.max(filtered_logits, 1)\n        all_preds += preds.detach().cpu()\n        all_labels_ids += b_labels.detach().cpu()\n\n    # Convert accumulated predictions and labels to NumPy arrays\n    all_preds = torch.stack(all_preds).numpy()\n    all_labels_ids = torch.stack(all_labels_ids).numpy()\n\n    # Calculate accuracy and average test loss\n    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n    avg_test_loss = total_test_loss / len(test_dataloader)\n    avg_test_loss = avg_test_loss.item()\n\n    # Calculate F1 score\n    f1 = f1_score(all_labels_ids, all_preds, average='weighted')\n\n    return test_accuracy, avg_test_loss, f1","metadata":{"id":"H7R3tdRwswlL","execution":{"iopub.status.busy":"2024-02-03T23:40:31.931428Z","iopub.execute_input":"2024-02-03T23:40:31.932142Z","iopub.status.idle":"2024-02-03T23:40:31.957597Z","shell.execute_reply.started":"2024-02-03T23:40:31.932109Z","shell.execute_reply":"2024-02-03T23:40:31.956550Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 50% Labeled","metadata":{"id":"0uKRme3CtKvp"}},{"cell_type":"code","source":"train_dataloader_50, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 50, do_shuffle=True)\nnum_train_epochs = 1\ntest_acc2, f1_2 = train_2(bag_of_words_matrix, transformer, generator2_50, discriminator_50, train_dataloader_50,test_dataloader, gen_optimizer_50, dis_optimizer_50, bert_optimizer_50,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_2)\ntest_accuracies.append(test_acc2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gttySqB_tJsN","outputId":"d115da45-4199-4a46-c600-ada245a77c78","execution":{"iopub.status.busy":"2024-02-03T23:40:35.971137Z","iopub.execute_input":"2024-02-03T23:40:35.972118Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_5234/1107263763.py:52: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  label_mask_array = torch.tensor(label_mask_array)\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 1 ========\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 10% Labeled","metadata":{"id":"VfGNpWhwxzcy"}},{"cell_type":"code","source":"train_dataloader_10, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 10, do_shuffle=True)\nnum_train_epochs = 1\ntest_acc2, f1_2 = train_2(bag_of_words_matrix, transformer, generator2_10, discriminator_10, train_dataloader_10,test_dataloader, gen_optimizer_10, dis_optimizer_10, bert_optimizer_10,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_2)\ntest_accuracies.append(test_acc2)","metadata":{"id":"4cZ18ttguAQV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5% Labeled","metadata":{"id":"uhD1xezryES7"}},{"cell_type":"code","source":"train_dataloader_5, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 5, do_shuffle=True)\nnum_train_epochs = 1\ntest_acc2, f1_2 = train_2(bag_of_words_matrix, transformer, generator2_5, discriminator_5, train_dataloader_5,test_dataloader, gen_optimizer_5, dis_optimizer_5, bert_optimizer_5,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_2)\ntest_accuracies.append(test_acc2)","metadata":{"id":"8FJraJixyDx4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1% Labeled","metadata":{"id":"HDbtOg43yOMi"}},{"cell_type":"code","source":"train_dataloader_1, bag_of_words_matrix = load_and_generate_data_loader(train_file_path, label_map, 1, do_shuffle=True)\nnum_train_epochs = 1\ntest_acc2, f1_2 = train_2(bag_of_words_matrix, transformer, generator2_1, discriminator_1, train_dataloader_1,test_dataloader, gen_optimizer_1, dis_optimizer_1, bert_optimizer_1,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_2)\ntest_accuracies.append(test_acc2)","metadata":{"id":"wlmAeo0UyPTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming the dataloader indices are 50, 10, 5, and 1\ndataloader_indices = [50, 10, 5, 1]\n\nplt.figure(figsize=(10, 5))\n\n# Plotting Test Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(dataloader_indices, test_accuracies, marker='o')\nplt.title('Test Accuracy vs. Dataloader Index')\nplt.xlabel('Dataloader Index')\nplt.ylabel('Test Accuracy')\n\n# Plotting F1 Score\nplt.subplot(1, 2, 2)\nplt.plot(dataloader_indices, f1_scores, marker='o')\nplt.title('F1 Score vs. Dataloader Index')\nplt.xlabel('Dataloader Index')\nplt.ylabel('F1 Score')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}