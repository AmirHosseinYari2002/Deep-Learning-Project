{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_JnE73nnYep"
      },
      "source": [
        "<h1 align=\"center\">Deep Learning Project</h1>\n",
        "<h4 align=\"center\">Dr. Fatemizadeh</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Fall 2023</h4>\n",
        "<h4 align=\"center\">Amir Hossein Yari - Mohammad Taslimi - Mahdi Heidari</h4>\n",
        "<h4 align=\"center\">99102507 - 99101321 - 99100369</h4>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Packages\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import json\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoConfig\n",
        "import time\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "LabxVyN0oapL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3"
      ],
      "metadata": {
        "id": "yLLlzUBDndrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op5cyBoxDg1j",
        "outputId": "07ee9698-732e-43c1-ba2f-1bbebd99453d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------\n",
        "#   Generator 1\n",
        "#------------------------------\n",
        "class Generator1(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=768, hidden_size=768, dropout_rate=0.1):\n",
        "        super(Generator1, self).__init__()\n",
        "\n",
        "        # Build layers sequentially\n",
        "        layers = []\n",
        "\n",
        "        # Linear transformation from noise to hidden_size\n",
        "        layers.append(nn.Linear(noise_size, hidden_size))\n",
        "        # Leaky ReLU activation\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        # Dropout layer\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(hidden_size, output_size))\n",
        "\n",
        "        # Define the sequential model\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        # Forward pass through the layers\n",
        "        output = self.layers(noise)\n",
        "        return output"
      ],
      "metadata": {
        "id": "rc1lPwBOnZta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------\n",
        "#   Generator 2\n",
        "#------------------------------\n",
        "class Generator2(nn.Module):\n",
        "    def __init__(self, bert_model, output_size=768, noise_size=100):\n",
        "        super(Generator2, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.output_size = output_size\n",
        "        self.noise_size = noise_size\n",
        "\n",
        "    def forward(self, bag_of_words):\n",
        "        # Random noise\n",
        "        noise = torch.randn((bag_of_words.size(0), self.noise_size))\n",
        "\n",
        "        # Concatenate Bag of Words and Noise\n",
        "        input_noise = torch.cat((bag_of_words, noise), dim=1)\n",
        "\n",
        "        # BERT-based feature extraction\n",
        "        bert_output = self.bert_model(input_noise)\n",
        "\n",
        "        # Extract the pooled output (CLS token) from BERT\n",
        "        pooled_output = bert_output.pooler_output\n",
        "\n",
        "        # Linear transformation to the desired output size\n",
        "        output = nn.Linear(pooled_output.size(1), self.output_size)(pooled_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "wuGJalQOuf1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_size=768, num_labels=6, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Input dropout\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Feature extraction layer\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Output layer for classification\n",
        "        self.logit = nn.Linear(hidden_size, num_labels + 1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Apply input dropout\n",
        "        input = self.input_dropout(input)\n",
        "\n",
        "        # Forward pass through feature extraction layers\n",
        "        feature = self.feature_layer(input)\n",
        "\n",
        "        # Output layer\n",
        "        logits = self.logit(feature)\n",
        "        probs = self.softmax(logits)\n",
        "\n",
        "        return feature, logits, probs"
      ],
      "metadata": {
        "id": "UWGo1luAruFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "vu24AMdGwbGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU) is available, and assign the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3G1oRY6xwema"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-trained BERT model name\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Load the corresponding tokenizer for BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "jnBKO79Fzi76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_question_classification_file(input_file):\n",
        "    examples = []\n",
        "\n",
        "    with open(input_file, 'r') as f:\n",
        "        # Iterate through each line in the JSONL file\n",
        "        for line in f:\n",
        "            # Parse the JSON from each line\n",
        "            data = json.loads(line)\n",
        "\n",
        "            # Extract relevant information\n",
        "            text = data.get('text', '')\n",
        "            label = data.get('label', '')\n",
        "            model = data.get('model', '')\n",
        "\n",
        "            # Append the tuple (text, label, model) to examples list\n",
        "            examples.append((text, label, model))\n",
        "\n",
        "    return examples"
      ],
      "metadata": {
        "id": "ODlhS5Fb4Jry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data_loader(examples, label_map, max_seq_length=64, batch_size=64, do_shuffle=False):\n",
        "    input_ids = []\n",
        "    input_mask_array = []\n",
        "    label_id_array = []\n",
        "\n",
        "    # Tokenization\n",
        "    for (text, label, model) in examples:\n",
        "        # Assuming tokenizer is predefined\n",
        "        encoded_sent = tokenizer.encode(text, add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "        input_ids.append(encoded_sent)\n",
        "        label_id_array.append(label_map.get(model))\n",
        "\n",
        "    # Attention mask\n",
        "    input_mask_array = [[int(token_id > 0) for token_id in sent] for sent in input_ids]\n",
        "\n",
        "    # Convert to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    input_mask_array = torch.tensor(input_mask_array)\n",
        "    label_id_array = torch.tensor(label_id_array)\n",
        "\n",
        "    # Building the TensorDataset\n",
        "    dataset = TensorDataset(input_ids, input_mask_array, label_id_array)\n",
        "\n",
        "    # Choose sampler based on shuffle option\n",
        "    sampler = RandomSampler if do_shuffle else SequentialSampler\n",
        "\n",
        "    # Building the DataLoader\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        sampler=sampler(dataset),\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "# Load the examples\n",
        "train_examples = parse_question_classification_file(\"/content/drive/MyDrive/Dataset/subtaskB_train.jsonl\")\n",
        "test_examples = parse_question_classification_file(\"/content/drive/MyDrive/Dataset/subtaskB_dev.jsonl\")\n",
        "\n",
        "# Assuming label_map is predefined\n",
        "label_map = {\"human\": 0, \"chatGPT\": 1, \"cohere\": 2, \"davinci\": 3, \"bloomz\": 4, \"dolly\": 5}\n",
        "\n",
        "# Generate DataLoaders for training and testing\n",
        "train_dataloader = generate_data_loader(train_examples, label_map, do_shuffle=True)\n",
        "test_dataloader = generate_data_loader(test_examples, label_map, do_shuffle=False)"
      ],
      "metadata": {
        "id": "W_gEiEDE9bv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Generator and Discriminator\n",
        "generator1 = Generator1()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Put everything on the GPU if available\n",
        "generator1.to(device)\n",
        "discriminator.to(device)\n",
        "transformer.to(device)\n",
        "\n",
        "multi_gpu = False\n",
        "\n",
        "# Use DataParallel if multi_gpu is True\n",
        "if multi_gpu and torch.cuda.is_available():\n",
        "    generator1 = torch.nn.DataParallel(generator1)\n",
        "    discriminator = torch.nn.DataParallel(discriminator)"
      ],
      "metadata": {
        "id": "o-sBuIkkAlSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Hyperparameters\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8\n",
        "num_train_epochs = 10\n",
        "print_each_n_step = 100\n",
        "\n",
        "# Extract model parameters for Discriminator, Generator, and Transformer\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator1.parameters()]\n",
        "\n",
        "# Set up optimizers for Discriminator and Generator\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator)"
      ],
      "metadata": {
        "id": "INySkcGdD-z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch_i in range(num_train_epochs):\n",
        "    print(f\"\\n======== Epoch {epoch_i + 1} / {num_train_epochs} ========\")\n",
        "\n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "\n",
        "    # Set models to training mode\n",
        "    transformer.train()\n",
        "    generator1.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # Iterate through batches in the training dataloader\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Display progress every print_each_n_step batches\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            print(f\"  Batch {step:>5,}  of  {len(train_dataloader):>5,}.\")\n",
        "\n",
        "        # Move batch tensors to device\n",
        "        b_input_ids, b_input_mask, b_labels = [tensor.to(device) for tensor in batch]\n",
        "        real_batch_size = b_input_ids.shape[0]\n",
        "\n",
        "        # Forward pass through the transformer\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "\n",
        "        # Generate fake data using the generator\n",
        "        noise = torch.zeros(real_batch_size, 100, device=device).uniform_(0, 1)\n",
        "        gen_rep = generator1(noise)\n",
        "\n",
        "        # Concatenate real and fake data for the discriminator input\n",
        "        discriminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "        features, logits, probs = discriminator(discriminator_input)\n",
        "\n",
        "        # Split features, logits, and probs for real and fake data\n",
        "        features_list = torch.split(features, real_batch_size)\n",
        "        D_real_features, D_fake_features = features_list[0], features_list[1]\n",
        "\n",
        "        logits_list = torch.split(logits, real_batch_size)\n",
        "        D_real_logits, D_fake_logits = logits_list[0], logits_list[1]\n",
        "\n",
        "        probs_list = torch.split(probs, real_batch_size)\n",
        "        D_real_probs, D_fake_probs = probs_list[0], probs_list[1]\n",
        "\n",
        "        # Generator's loss calculation\n",
        "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:, -1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "\n",
        "        # Discriminator's loss calculation\n",
        "        logits = D_real_logits[:, 0:-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_map))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "        D_L_Supervised = 0 if labeled_example_count == 0 else torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward()\n",
        "        gen_optimizer.step()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "        # Update loss accumulators\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "    # Calculate average training losses\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)\n",
        "\n",
        "    # Print average training losses for the epoch\n",
        "    print(\"\\n  Average training loss generator: {:.3f}\".format(avg_train_loss_g))\n",
        "    print(\"  Average training loss discriminator: {:.3f}\".format(avg_train_loss_d))"
      ],
      "metadata": {
        "id": "ABW9tkTnEQC0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90bc110c-c214-41f4-b400-56dbbe4e1a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "  Batch   100  of  1,110.\n",
            "  Batch   200  of  1,110.\n",
            "  Batch   300  of  1,110.\n",
            "  Batch   400  of  1,110.\n",
            "  Batch   500  of  1,110.\n",
            "  Batch   600  of  1,110.\n",
            "  Batch   700  of  1,110.\n",
            "  Batch   800  of  1,110.\n",
            "  Batch   900  of  1,110.\n",
            "  Batch 1,000  of  1,110.\n",
            "  Batch 1,100  of  1,110.\n",
            "\n",
            "  Average training loss generator: 0.702\n",
            "  Average training loss discriminator: 1.625\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "  Batch   100  of  1,110.\n",
            "  Batch   200  of  1,110.\n",
            "  Batch   300  of  1,110.\n",
            "  Batch   400  of  1,110.\n",
            "  Batch   500  of  1,110.\n",
            "  Batch   600  of  1,110.\n",
            "  Batch   700  of  1,110.\n",
            "  Batch   800  of  1,110.\n",
            "  Batch   900  of  1,110.\n",
            "  Batch 1,000  of  1,110.\n",
            "  Batch 1,100  of  1,110.\n",
            "\n",
            "  Average training loss generator: 0.700\n",
            "  Average training loss discriminator: 1.201\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "  Batch   100  of  1,110.\n",
            "  Batch   200  of  1,110.\n",
            "  Batch   300  of  1,110.\n",
            "  Batch   400  of  1,110.\n",
            "  Batch   500  of  1,110.\n",
            "  Batch   600  of  1,110.\n",
            "  Batch   700  of  1,110.\n",
            "  Batch   800  of  1,110.\n",
            "  Batch   900  of  1,110.\n",
            "  Batch 1,000  of  1,110.\n",
            "  Batch 1,100  of  1,110.\n",
            "\n",
            "  Average training loss generator: 0.700\n",
            "  Average training loss discriminator: 1.005\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "  Batch   100  of  1,110.\n",
            "  Batch   200  of  1,110.\n",
            "  Batch   300  of  1,110.\n",
            "  Batch   400  of  1,110.\n",
            "  Batch   500  of  1,110.\n",
            "  Batch   600  of  1,110.\n",
            "  Batch   700  of  1,110.\n",
            "  Batch   800  of  1,110.\n",
            "  Batch   900  of  1,110.\n",
            "  Batch 1,000  of  1,110.\n",
            "  Batch 1,100  of  1,110.\n",
            "\n",
            "  Average training loss generator: 0.699\n",
            "  Average training loss discriminator: 0.881\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "  Batch   100  of  1,110.\n",
            "  Batch   200  of  1,110.\n",
            "  Batch   300  of  1,110.\n",
            "  Batch   400  of  1,110.\n",
            "  Batch   500  of  1,110.\n",
            "  Batch   600  of  1,110.\n",
            "  Batch   700  of  1,110.\n",
            "  Batch   800  of  1,110.\n",
            "  Batch   900  of  1,110.\n",
            "  Batch 1,000  of  1,110.\n",
            "  Batch 1,100  of  1,110.\n",
            "\n",
            "  Average training loss generator: 0.698\n",
            "  Average training loss discriminator: 0.817\n",
            "\n",
            "======== Epoch 6 / 10 ========\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8cc693d602ff>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Update loss accumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mtr_g_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mtr_d_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set models to evaluation mode\n",
        "transformer.eval()\n",
        "discriminator.eval()\n",
        "generator1.eval()\n",
        "\n",
        "# Initialize variables for test evaluation\n",
        "total_test_accuracy = 0\n",
        "total_test_loss = 0\n",
        "nb_test_steps = 0\n",
        "\n",
        "all_preds = []\n",
        "all_labels_ids = []\n",
        "\n",
        "# Define the loss function for evaluation\n",
        "nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "# Iterate through the test dataloader\n",
        "for batch in test_dataloader:\n",
        "    # Move batch tensors to the device (GPU or CPU)\n",
        "    b_input_ids, b_input_mask, b_labels = [tensor.to(device) for tensor in batch]\n",
        "\n",
        "    # Perform forward pass without gradient computation\n",
        "    with torch.no_grad():\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        _, logits, probs = discriminator(hidden_states)\n",
        "\n",
        "        # Extract logits for labeled classes (excluding the fake class)\n",
        "        filtered_logits = logits[:, 0:-1]\n",
        "        total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "\n",
        "    # Calculate accuracy and accumulate predictions and labels\n",
        "    _, preds = torch.max(filtered_logits, 1)\n",
        "    all_preds += preds.detach().cpu()\n",
        "    all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "# Convert accumulated predictions and labels to NumPy arrays\n",
        "all_preds = torch.stack(all_preds).numpy()\n",
        "all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "\n",
        "# Calculate accuracy and average test loss\n",
        "test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "avg_test_loss = avg_test_loss.item()\n",
        "\n",
        "# Print test evaluation results\n",
        "print(\"\\n  Accuracy on Test Set: {:.3f}\".format(test_accuracy))\n",
        "print(\"  Average Test Loss: {:.3f}\".format(avg_test_loss))"
      ],
      "metadata": {
        "id": "PEuwBsf-F940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490c0d85-b68b-4c77-f66e-bb36c80c04c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Accuracy on Test Set: 0.527\n",
            "  Average Test Loss: 2.043\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}