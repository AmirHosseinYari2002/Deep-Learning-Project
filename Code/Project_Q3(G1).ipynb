{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install -y gdown","metadata":{"execution":{"iopub.status.busy":"2024-02-03T14:28:11.749711Z","iopub.execute_input":"2024-02-03T14:28:11.750098Z","iopub.status.idle":"2024-02-03T14:34:48.025869Z","shell.execute_reply.started":"2024-02-03T14:28:11.750070Z","shell.execute_reply":"2024-02-03T14:34:48.024595Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): \\ WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\ndone\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 23.7.4\n  latest version: 23.11.0\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=23.11.0\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2024.2.2   |       hbcca054_0         152 KB  conda-forge\n    certifi-2023.11.17         |     pyhd8ed1ab_1         155 KB  conda-forge\n    filelock-3.13.1            |     pyhd8ed1ab_0          15 KB  conda-forge\n    gdown-5.0.1                |     pyhd8ed1ab_0          20 KB  conda-forge\n    openssl-3.2.1              |       hd590300_0         2.7 MB  conda-forge\n    scipy-1.12.0               |  py310hb13e2d6_2        15.7 MB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        18.8 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.13.1-pyhd8ed1ab_0 \n  gdown              conda-forge/noarch::gdown-5.0.1-pyhd8ed1ab_0 \n\nThe following packages will be UPDATED:\n\n  ca-certificates                     2023.11.17-hbcca054_0 --> 2024.2.2-hbcca054_0 \n  certifi                           2023.11.17-pyhd8ed1ab_0 --> 2023.11.17-pyhd8ed1ab_1 \n  openssl                                  3.2.0-hd590300_1 --> 3.2.1-hd590300_0 \n  scipy                              1.12.0-py310hb13e2d6_1 --> 1.12.0-py310hb13e2d6_2 \n\n\n\nDownloading and Extracting Packages\ngdown-5.0.1          | 20 KB     |                                       |   0% \nscipy-1.12.0         | 15.7 MB   |                                       |   0% \u001b[A\n\ncertifi-2023.11.17   | 155 KB    |                                       |   0% \u001b[A\u001b[A\n\n\nfilelock-3.13.1      | 15 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n\n\n\nopenssl-3.2.1        | 2.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\ngdown-5.0.1          | 20 KB     | #############################6        |  80% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\ncertifi-2023.11.17   | 155 KB    | ###8                                  |  10% \u001b[A\u001b[A\n\n\n\nopenssl-3.2.1        | 2.7 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\nfilelock-3.13.1      | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\nscipy-1.12.0         | 15.7 MB   |                                       |   0% \u001b[A\n\n\n\n\ngdown-5.0.1          | 20 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nfilelock-3.13.1      | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n\n\n\nopenssl-3.2.1        | 2.7 MB    | #################5                    |  47% \u001b[A\u001b[A\u001b[A\u001b[A\nscipy-1.12.0         | 15.7 MB   | ##7                                   |   7% \u001b[A\n\ncertifi-2023.11.17   | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\n\ncertifi-2023.11.17   | 155 KB    | ##################################### | 100% \u001b[A\u001b[A\n\n\n\n\nca-certificates-2024 | 152 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nscipy-1.12.0         | 15.7 MB   | #################1                    |  46% \u001b[A\n\n\n\nopenssl-3.2.1        | 2.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nopenssl-3.2.1        | 2.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\nscipy-1.12.0         | 15.7 MB   | ####################################4 |  98% \u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\n\n                                                                                \u001b[A\u001b[A\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\n\n                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown 1vXuN6h5pMha_dmU_mWhn2e1fsCANGoOT\n!gdown 1HxoefWIplFdkKT6mPOFTO-aIrE788jRN","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:37:49.244665Z","iopub.execute_input":"2024-02-03T16:37:49.245655Z","iopub.status.idle":"2024-02-03T16:37:58.804751Z","shell.execute_reply.started":"2024-02-03T16:37:49.245618Z","shell.execute_reply":"2024-02-03T16:37:58.803525Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1vXuN6h5pMha_dmU_mWhn2e1fsCANGoOT\nFrom (redirected): https://drive.google.com/uc?id=1vXuN6h5pMha_dmU_mWhn2e1fsCANGoOT&confirm=t&uuid=5c0fce17-38e3-46ea-ac55-3993931b1bfa\nTo: /kaggle/working/subtaskB_train.jsonl\n100%|█████████████████████████████████████████| 155M/155M [00:00<00:00, 223MB/s]\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1HxoefWIplFdkKT6mPOFTO-aIrE788jRN\nTo: /kaggle/working/subtaskB_dev.jsonl\n100%|███████████████████████████████████████| 4.93M/4.93M [00:00<00:00, 231MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Required Packages\nimport torch.nn as nn\nimport torch\nimport random\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer\nimport json\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AutoConfig\nimport time\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-02-03T21:14:15.321258Z","iopub.execute_input":"2024-02-03T21:14:15.321974Z","iopub.status.idle":"2024-02-03T21:14:15.327690Z","shell.execute_reply.started":"2024-02-03T21:14:15.321943Z","shell.execute_reply":"2024-02-03T21:14:15.326706Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n#   Generator 1\n#------------------------------\nclass Generator1(nn.Module):\n    def __init__(self, noise_size=100, output_size=768, hidden_size=768, dropout_rate=0.8):\n        super(Generator1, self).__init__()\n\n        # Build layers sequentially\n        layers = []\n\n        # Linear transformation from noise to hidden_size\n        layers.append(nn.Linear(noise_size, hidden_size))\n        # Leaky ReLU activation\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        # Dropout layer\n        layers.append(nn.Dropout(dropout_rate))\n\n        # Output layer\n        layers.append(nn.Linear(hidden_size, output_size))\n\n        # Define the sequential model\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, noise):\n        # Forward pass through the layers\n        output = self.layers(noise)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:38:06.153651Z","iopub.execute_input":"2024-02-03T16:38:06.154636Z","iopub.status.idle":"2024-02-03T16:38:06.161917Z","shell.execute_reply.started":"2024-02-03T16:38:06.154598Z","shell.execute_reply":"2024-02-03T16:38:06.161043Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n#   Generator 2\n#------------------------------\nclass Generator2(nn.Module):\n    def __init__(self, bert_model, output_size=768, noise_size=100):\n        super(Generator2, self).__init__()\n        self.bert_model = bert_model\n        self.output_size = output_size\n        self.noise_size = noise_size\n\n    def forward(self, bag_of_words):\n        # Random noise\n        noise = torch.randn((bag_of_words.size(0), self.noise_size))\n\n        # Concatenate Bag of Words and Noise\n        input_noise = torch.cat((bag_of_words, noise), dim=1)\n\n        # BERT-based feature extraction\n        bert_output = self.bert_model(input_noise)\n\n        # Extract the pooled output (CLS token) from BERT\n        pooled_output = bert_output.pooler_output\n\n        # Linear transformation to the desired output size\n        output = nn.Linear(pooled_output.size(1), self.output_size)(pooled_output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-03T09:25:59.089388Z","iopub.execute_input":"2024-02-03T09:25:59.090298Z","iopub.status.idle":"2024-02-03T09:25:59.097460Z","shell.execute_reply.started":"2024-02-03T09:25:59.090253Z","shell.execute_reply":"2024-02-03T09:25:59.096581Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n#   The Discriminator\n#------------------------------\nclass Discriminator(nn.Module):\n    def __init__(self, input_size=768, hidden_size=768, num_labels=6, dropout_rate=0.8):\n        super(Discriminator, self).__init__()\n\n        # Input dropout\n        self.input_dropout = nn.Dropout(p=dropout_rate)\n\n        # Feature extraction layer\n        self.feature_layer = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(dropout_rate)\n        )\n\n        # Output layer for classification\n        self.logit = nn.Linear(hidden_size, num_labels + 1) # +1 for the probability of this sample being fake/real.\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, input):\n        # Apply input dropout\n        input = self.input_dropout(input)\n\n        # Forward pass through feature extraction layers\n        feature = self.feature_layer(input)\n\n        # Output layer\n        logits = self.logit(feature)\n        probs = self.softmax(logits)\n\n        return feature, logits, probs","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:38:11.053469Z","iopub.execute_input":"2024-02-03T16:38:11.054504Z","iopub.status.idle":"2024-02-03T16:38:11.063618Z","shell.execute_reply.started":"2024-02-03T16:38:11.054457Z","shell.execute_reply":"2024-02-03T16:38:11.062583Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"##Set random values\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\nif torch.cuda.is_available():\n  torch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:38:18.291684Z","iopub.execute_input":"2024-02-03T16:38:18.292330Z","iopub.status.idle":"2024-02-03T16:38:18.297875Z","shell.execute_reply.started":"2024-02-03T16:38:18.292299Z","shell.execute_reply":"2024-02-03T16:38:18.296881Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Check if CUDA (GPU) is available, and assign the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Pre-trained BERT model name\nmodel_name = \"bert-base-cased\"\n\n# Load the pre-trained BERT model\ntransformer = AutoModel.from_pretrained(model_name)\n\n# Load the corresponding tokenizer for BERT\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T16:38:20.843159Z","iopub.execute_input":"2024-02-03T16:38:20.844058Z","iopub.status.idle":"2024-02-03T16:38:21.279336Z","shell.execute_reply.started":"2024-02-03T16:38:20.844017Z","shell.execute_reply":"2024-02-03T16:38:21.278271Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def parse_question_classification_file(input_file):\n    examples = []\n\n    with open(input_file, 'r') as f:\n        # Iterate through each line in the JSONL file\n        for line in f:\n            # Parse the JSON from each line\n            data = json.loads(line)\n\n            # Extract relevant information\n            text = data.get('text', '')\n            label = data.get('label', '')\n            model = data.get('model', '')\n\n            examples.append((text, label, model))\n\n    return examples\n\n\n\ndef generate_data_loader(examples, label_masks, label_map, max_seq_length=180, batch_size=32, do_shuffle=False):\n    new_example = []\n    for index, ex in enumerate(examples):\n      new_example.append((ex, label_masks[index]))\n\n    input_ids = []\n    input_mask_array = []\n    label_mask_array = []\n    label_id_array = []\n\n    # Tokenization\n    for (ex, label_mask) in new_example:\n        # Assuming tokenizer is predefined\n        if ex[0] == '':\n          print(\"qq\")\n        encoded_sent = tokenizer.encode(ex[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n        input_ids.append(encoded_sent)\n        label_id_array.append(label_map.get(ex[2]))\n        label_mask_array.append(label_mask)\n\n    # Attention mask\n    input_mask_array = [[int(token_id > 0) for token_id in sent] for sent in input_ids]\n\n    # Convert to tensors\n    input_ids = torch.tensor(input_ids)\n    input_mask_array = torch.tensor(input_mask_array)\n    label_id_array = torch.tensor(label_id_array)\n    label_mask_array = torch.tensor(label_mask_array)\n\n    # Building the TensorDataset\n    dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n\n    # Choose sampler based on shuffle option\n    sampler = RandomSampler if do_shuffle else SequentialSampler\n\n    # Building the DataLoader\n    return DataLoader(\n        dataset,\n        sampler=sampler(dataset),\n        batch_size=batch_size\n    )\n\n\n\n\ndef load_and_generate_data_loader(file_path, label_map, percentage_labeled, do_shuffle=False):\n    # Load the examples\n    examples = parse_question_classification_file(file_path)\n\n    # Calculate the number of examples to be labeled\n    num_labeled_examples = int(len(examples) * (percentage_labeled / 100))\n\n    # Split the examples into labeled and unlabeled\n    labeled_examples = examples[:num_labeled_examples]\n    unlabeled_examples = examples[num_labeled_examples:]\n\n    # The labeled dataset is assigned with a mask set to True\n    label_masks = np.ones(len(labeled_examples), dtype=bool)\n\n    # If unlabeled examples are available\n    if unlabeled_examples:\n        labeled_examples = labeled_examples + unlabeled_examples\n        # The unlabeled dataset is assigned with a mask set to False\n        tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n        label_masks = np.concatenate([label_masks, tmp_masks])\n\n    # Generate DataLoaders\n    dataloader = generate_data_loader(labeled_examples, label_masks, label_map, do_shuffle=do_shuffle)\n\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:14:27.875555Z","iopub.execute_input":"2024-02-03T17:14:27.876306Z","iopub.status.idle":"2024-02-03T17:14:27.893410Z","shell.execute_reply.started":"2024-02-03T17:14:27.876271Z","shell.execute_reply":"2024-02-03T17:14:27.892344Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# Define the file paths\ntrain_file_path = \"subtaskB_train.jsonl\"\ntest_file_path = \"subtaskB_dev.jsonl\"\n\n# Assuming label_map is predefined\nlabel_map = {\"human\": 0, \"chatGPT\": 1, \"cohere\": 2, \"davinci\": 3, \"bloomz\": 4, \"dolly\": 5}\n\n# # Generate DataLoaders for training and testing for each percentage\n\ntrain_dataloader_50 = load_and_generate_data_loader(train_file_path, label_map, 50, do_shuffle=True)\ntest_dataloader = load_and_generate_data_loader(test_file_path, label_map, 100, do_shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:14:30.844996Z","iopub.execute_input":"2024-02-03T17:14:30.845344Z","iopub.status.idle":"2024-02-03T17:16:42.962259Z","shell.execute_reply.started":"2024-02-03T17:14:30.845320Z","shell.execute_reply":"2024-02-03T17:16:42.961461Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1707988561.py:48: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  label_mask_array = torch.tensor(label_mask_array)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Instantiate the Generator and Discriminator\ngenerator1_1 = Generator1()\ngenerator1_5 = Generator1()\ngenerator1_10 = Generator1()\ngenerator1_50 = Generator1()\ndiscriminator_1 = Discriminator()\ndiscriminator_5 = Discriminator()\ndiscriminator_10 = Discriminator()\ndiscriminator_50 = Discriminator()\n\n# Put everything on the GPU if available\ngenerator1_1.to(device)\ngenerator1_5.to(device)\ngenerator1_10.to(device)\ngenerator1_50.to(device)\ndiscriminator_1.to(device)\ndiscriminator_5.to(device)\ndiscriminator_10.to(device)\ndiscriminator_50.to(device)\ntransformer.to(device)\n\nmulti_gpu = False\n\n# Use DataParallel if multi_gpu is True\nif multi_gpu and torch.cuda.is_available():\n    generator1_1 = torch.nn.DataParallel(generator1_1)\n    generator1_5 = torch.nn.DataParallel(generator1_5)\n    generator1_10 = torch.nn.DataParallel(generator1_10)\n    generator1_50 = torch.nn.DataParallel(generator1_50)\n    discriminator_1 = torch.nn.DataParallel(discriminator_1)\n    discriminator_5 = torch.nn.DataParallel(discriminator_5)\n    discriminator_10 = torch.nn.DataParallel(discriminator_10)\n    discriminator_50 = torch.nn.DataParallel(discriminator_50)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:16:42.964731Z","iopub.execute_input":"2024-02-03T17:16:42.965024Z","iopub.status.idle":"2024-02-03T17:16:43.030548Z","shell.execute_reply.started":"2024-02-03T17:16:42.965000Z","shell.execute_reply":"2024-02-03T17:16:43.029725Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# Set Hyperparameters\nlearning_rate_discriminator = 5e-5\nlearning_rate_generator = 7e-4\nepsilon = 1e-8\nnum_train_epochs = 5\nprint_each_n_step = 100\n\n# Extract model parameters for Discriminator, Generator, and Transformer\ntransformer_vars = [i for i in transformer.parameters()]\nd_vars_1 = [v for v in discriminator_1.parameters()]\nd_vars_5 = [v for v in discriminator_5.parameters()]\nd_vars_10 = [v for v in discriminator_10.parameters()]\nd_vars_50 = [v for v in discriminator_50.parameters()]\ng_vars_1 = [v for v in generator1_1.parameters()]\ng_vars_5 = [v for v in generator1_5.parameters()]\ng_vars_10 = [v for v in generator1_10.parameters()]\ng_vars_50 = [v for v in generator1_50.parameters()]\n\n# Set up optimizers for Discriminator and Generator\ndis_optimizer_1 = torch.optim.AdamW(d_vars_1, lr=learning_rate_generator)\ndis_optimizer_5 = torch.optim.AdamW(d_vars_5, lr=learning_rate_generator)\ndis_optimizer_10 = torch.optim.AdamW(d_vars_10, lr=learning_rate_generator)\ndis_optimizer_50 = torch.optim.AdamW(d_vars_50, lr=learning_rate_generator)\ngen_optimizer_1 = torch.optim.AdamW(g_vars_1, lr=learning_rate_generator)\ngen_optimizer_5 = torch.optim.AdamW(g_vars_5, lr=learning_rate_generator)\ngen_optimizer_10 = torch.optim.AdamW(g_vars_10, lr=learning_rate_generator)\ngen_optimizer_50 = torch.optim.AdamW(g_vars_50, lr=learning_rate_generator)\n\n\n\n\nbert_optimizer_1 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_5 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_10 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)\nbert_optimizer_50 = torch.optim.AdamW(transformer_vars, lr=learning_rate_discriminator)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:16:43.031657Z","iopub.execute_input":"2024-02-03T17:16:43.031959Z","iopub.status.idle":"2024-02-03T17:16:43.047560Z","shell.execute_reply.started":"2024-02-03T17:16:43.031935Z","shell.execute_reply":"2024-02-03T17:16:43.046814Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"## Train with 50 % labeled data","metadata":{}},{"cell_type":"code","source":"test_accuracies = []\nf1_scores = []\n\n    \ndef train(transformer, generator1, discriminator, train_dataloader,test_dataloader, gen_optimizer, dis_optimizer, bert_optimizer, device, num_train_epochs, print_each_n_step=100, epsilon=1e-12):\n\n    test_accuracy = 0\n    f1 = 0\n\n    for epoch_i in range(num_train_epochs):\n        print(f\"\\n======== Epoch {epoch_i + 1} / {num_train_epochs} ========\")\n\n        tr_g_loss = 0\n        tr_d_loss = 0\n\n        # Set models to training mode\n        transformer.train()\n        generator1.train()\n        discriminator.train()\n\n        # Iterate through batches in the training dataloader\n        for step, batch in enumerate(train_dataloader):\n            # Display progress every print_each_n_step batches\n            if step % print_each_n_step == 0 and not step == 0:\n                print(f\"  Batch {step:>5,}  of  {len(train_dataloader):>5,}.\")\n\n            # Move batch tensors to device\n            b_input_ids, b_input_mask, b_labels, b_label_mask = [tensor.to(device) for tensor in batch]\n            real_batch_size = b_input_ids.shape[0]\n\n            # Forward pass through the transformer\n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n\n            # Generate fake data using the generator\n            noise = torch.zeros(real_batch_size, 100, device=device).uniform_(0, 1)\n            gen_rep = generator1(noise)\n\n            # Concatenate real and fake data for the discriminator input\n            discriminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n            features, logits, probs = discriminator(discriminator_input)\n\n            # Split features, logits, and probs for real and fake data\n            features_list = torch.split(features, real_batch_size)\n            D_real_features, D_fake_features = features_list[0], features_list[1]\n\n            logits_list = torch.split(logits, real_batch_size)\n            D_real_logits, D_fake_logits = logits_list[0], logits_list[1]\n\n            probs_list = torch.split(probs, real_batch_size)\n            D_real_probs, D_fake_probs = probs_list[0], probs_list[1]\n\n            # Generator's loss calculation\n            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:, -1] + epsilon))\n            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n            g_loss = g_loss_d + g_feat_reg\n\n            # Discriminator's loss calculation\n            logits = D_real_logits[:, 0:-1]\n            log_probs = F.log_softmax(logits, dim=-1)\n            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_map))\n            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n            labeled_example_count = per_example_loss.type(torch.float32).numel()\n\n            D_L_Supervised = 0 if labeled_example_count == 0 else torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n\n            # Backward pass and optimization\n            gen_optimizer.zero_grad()\n            dis_optimizer.zero_grad()\n            bert_optimizer.zero_grad()\n            g_loss.backward(retain_graph=True)\n            d_loss.backward()\n            gen_optimizer.step()\n            dis_optimizer.step()\n            bert_optimizer.step()\n            \n\n            # Update loss accumulators\n            tr_g_loss += g_loss.item()\n            tr_d_loss += d_loss.item()\n\n        # Calculate average training losses\n        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n        avg_train_loss_d = tr_d_loss / len(train_dataloader)\n\n        # Print average training losses for the epoch\n        print(\"\\n  Average training loss generator: {:.3f}\".format(avg_train_loss_g))\n        print(\"  Average training loss discriminator: {:.3f} \\n\\n\".format(avg_train_loss_d))\n        test_accuracy, avg_test_loss, f1 = test(transformer, discriminator, test_dataloader, device)\n        print(\"\\n  Accuracy on Test Set: {:.3f}\".format(test_accuracy))\n        print(\"  Average Test Loss: {:.3f}\".format(avg_test_loss))\n        print(\"  F1 Score on Test Set: {:.3f}\".format(f1))\n        \n        \n    return test_accuracy, f1\n\n\ndef test(transformer, discriminator, test_dataloader, device):\n\n\n    # Set models to evaluation mode\n    transformer.eval()\n    discriminator.eval()\n\n    # Initialize variables for test evaluation\n    total_test_accuracy = 0\n    total_test_loss = 0\n    nb_test_steps = 0\n\n    all_preds = []\n    all_labels_ids = []\n\n    # Define the loss function for evaluation\n    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n\n    # Iterate through the test dataloader\n    for batch in test_dataloader:\n        # Move batch tensors to the device (GPU or CPU)\n        b_input_ids, b_input_mask, b_labels, _ = [tensor.to(device) for tensor in batch]\n\n        # Perform forward pass without gradient computation\n        with torch.no_grad():\n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n            _, logits, probs = discriminator(hidden_states)\n\n            # Extract logits for labeled classes (excluding the fake class)\n            filtered_logits = logits[:, 0:-1]\n            total_test_loss += nll_loss(filtered_logits, b_labels)\n\n        # Calculate accuracy and accumulate predictions and labels\n        _, preds = torch.max(filtered_logits, 1)\n        all_preds += preds.detach().cpu()\n        all_labels_ids += b_labels.detach().cpu()\n\n    # Convert accumulated predictions and labels to NumPy arrays\n    all_preds = torch.stack(all_preds).numpy()\n    all_labels_ids = torch.stack(all_labels_ids).numpy()\n\n    # Calculate accuracy and average test loss\n    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n    avg_test_loss = total_test_loss / len(test_dataloader)\n    avg_test_loss = avg_test_loss.item()\n\n    # Calculate F1 score\n    f1 = f1_score(all_labels_ids, all_preds, average='weighted')\n\n    return test_accuracy, avg_test_loss, f1\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:16:43.049406Z","iopub.execute_input":"2024-02-03T17:16:43.049675Z","iopub.status.idle":"2024-02-03T17:16:43.077823Z","shell.execute_reply.started":"2024-02-03T17:16:43.049654Z","shell.execute_reply":"2024-02-03T17:16:43.077079Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"test_acc, f1 = train(transformer, generator1_50, discriminator_50, train_dataloader_50,test_dataloader, gen_optimizer_50, dis_optimizer_50, bert_optimizer_50,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1)\ntest_accuracies.append(test_acc)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T17:16:43.079017Z","iopub.execute_input":"2024-02-03T17:16:43.079677Z","iopub.status.idle":"2024-02-03T19:54:37.766917Z","shell.execute_reply.started":"2024-02-03T17:16:43.079646Z","shell.execute_reply":"2024-02-03T19:54:37.765824Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 5 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.740\n  Average training loss discriminator: 1.618 \n\n\n\n  Accuracy on Test Set: 0.478\n  Average Test Loss: 1.951\n  F1 Score on Test Set: 0.415\n\n======== Epoch 2 / 5 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.733\n  Average training loss discriminator: 1.200 \n\n\n\n  Accuracy on Test Set: 0.403\n  Average Test Loss: 2.229\n  F1 Score on Test Set: 0.368\n\n======== Epoch 3 / 5 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.732\n  Average training loss discriminator: 1.093 \n\n\n\n  Accuracy on Test Set: 0.471\n  Average Test Loss: 2.113\n  F1 Score on Test Set: 0.428\n\n======== Epoch 4 / 5 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.733\n  Average training loss discriminator: 0.998 \n\n\n\n  Accuracy on Test Set: 0.405\n  Average Test Loss: 2.839\n  F1 Score on Test Set: 0.347\n\n======== Epoch 5 / 5 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.731\n  Average training loss discriminator: 0.946 \n\n\n\n  Accuracy on Test Set: 0.444\n  Average Test Loss: 2.768\n  F1 Score on Test Set: 0.399\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train with 10 % labeled data","metadata":{}},{"cell_type":"code","source":"train_dataloader_10 = load_and_generate_data_loader(train_file_path, label_map, 10, do_shuffle=True)\nnum_train_epochs = 3\ntest_acc2, f1_2 = train(transformer, generator1_10, discriminator_10, train_dataloader_10,test_dataloader, gen_optimizer_10, dis_optimizer_10, bert_optimizer_10,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_2)\ntest_accuracies.append(test_acc2)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T19:56:32.785134Z","iopub.execute_input":"2024-02-03T19:56:32.786098Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1707988561.py:48: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  label_mask_array = torch.tensor(label_mask_array)\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 3 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.731\n  Average training loss discriminator: 1.152 \n\n\n\n  Accuracy on Test Set: 0.248\n  Average Test Loss: 2.637\n  F1 Score on Test Set: 0.170\n\n======== Epoch 2 / 3 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.735\n  Average training loss discriminator: 1.054 \n\n\n\n  Accuracy on Test Set: 0.265\n  Average Test Loss: 3.597\n  F1 Score on Test Set: 0.168\n\n======== Epoch 3 / 3 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train with 5 % labeled data","metadata":{}},{"cell_type":"code","source":"train_dataloader_5 = load_and_generate_data_loader(train_file_path, label_map, 5, do_shuffle=True)\nnum_train_epochs = 2\ntest_acc3, f1_3 = train(transformer, generator1_5, discriminator_5, train_dataloader_5,test_dataloader, gen_optimizer_5, dis_optimizer_5, bert_optimizer_5,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_3)\ntest_accuracies.append(test_acc3)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T21:14:27.034887Z","iopub.execute_input":"2024-02-03T21:14:27.035240Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1707988561.py:48: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n  label_mask_array = torch.tensor(label_mask_array)\n","output_type":"stream"},{"name":"stdout","text":"\n======== Epoch 1 / 2 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.736\n  Average training loss discriminator: 1.299 \n\n\n\n  Accuracy on Test Set: 0.167\n  Average Test Loss: 4.997\n  F1 Score on Test Set: 0.048\n\n======== Epoch 2 / 2 ========\n  Batch   100  of  2,220.\n  Batch   200  of  2,220.\n  Batch   300  of  2,220.\n  Batch   400  of  2,220.\n  Batch   500  of  2,220.\n  Batch   600  of  2,220.\n  Batch   700  of  2,220.\n  Batch   800  of  2,220.\n  Batch   900  of  2,220.\n  Batch 1,000  of  2,220.\n  Batch 1,100  of  2,220.\n  Batch 1,200  of  2,220.\n  Batch 1,300  of  2,220.\n  Batch 1,400  of  2,220.\n  Batch 1,500  of  2,220.\n  Batch 1,600  of  2,220.\n  Batch 1,700  of  2,220.\n  Batch 1,800  of  2,220.\n  Batch 1,900  of  2,220.\n  Batch 2,000  of  2,220.\n  Batch 2,100  of  2,220.\n  Batch 2,200  of  2,220.\n\n  Average training loss generator: 0.738\n  Average training loss discriminator: 1.333 \n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train with 1 % labeled data","metadata":{}},{"cell_type":"code","source":"train_dataloader_1 = load_and_generate_data_loader(train_file_path, label_map, 1, do_shuffle=True)\nnum_train_epochs = 1\ntest_acc4, f1_4 = train(transformer, generator1_5, discriminator_5, train_dataloader_5,test_dataloader, gen_optimizer_5, dis_optimizer_5, bert_optimizer_5,device, num_train_epochs, print_each_n_step=100, epsilon=1e-12)\nf1_scores.append(f1_4)\ntest_accuracies.append(test_acc4)","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:08:56.543809Z","iopub.execute_input":"2024-02-03T23:08:56.544419Z","iopub.status.idle":"2024-02-03T23:08:56.876989Z","shell.execute_reply.started":"2024-02-03T23:08:56.544386Z","shell.execute_reply":"2024-02-03T23:08:56.875521Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader_1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_generate_data_loader\u001b[49m(train_file_path, label_map, \u001b[38;5;241m1\u001b[39m, do_shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m num_train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m test_acc4, f1_4 \u001b[38;5;241m=\u001b[39m train(transformer, generator1_5, discriminator_5, train_dataloader_5,test_dataloader, gen_optimizer_5, dis_optimizer_5, bert_optimizer_5,device, num_train_epochs, print_each_n_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-12\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'load_and_generate_data_loader' is not defined"],"ename":"NameError","evalue":"name 'load_and_generate_data_loader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming the dataloader indices are 50, 10, 5, and 1\ndataloader_indices = [50, 10, 5, 1]\n\nplt.figure(figsize=(10, 5))\n\n# Plotting Test Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(dataloader_indices, test_accuracies, marker='o')\nplt.title('Test Accuracy vs. Dataloader Index')\nplt.xlabel('Dataloader Index')\nplt.ylabel('Test Accuracy')\n\n# Plotting F1 Score\nplt.subplot(1, 2, 2)\nplt.plot(dataloader_indices, f1_scores, marker='o')\nplt.title('F1 Score vs. Dataloader Index')\nplt.xlabel('Dataloader Index')\nplt.ylabel('F1 Score')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-03T23:09:09.545677Z","iopub.execute_input":"2024-02-03T23:09:09.546016Z","iopub.status.idle":"2024-02-03T23:09:09.747607Z","shell.execute_reply.started":"2024-02-03T23:09:09.545988Z","shell.execute_reply":"2024-02-03T23:09:09.746497Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Plotting Test Accuracy\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(dataloader_indices, \u001b[43mtest_accuracies\u001b[49m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy vs. Dataloader Index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataloader Index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'test_accuracies' is not defined"],"ename":"NameError","evalue":"name 'test_accuracies' is not defined","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAZ4AAAGyCAYAAADUEqJCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbEUlEQVR4nO3df2zV1f3H8Vdb6C1GWnBdb0t3tQPnT5RiK11BYlzubKKp44/FTgztGn9M7YxyswkVaEWUMqekiRSJqNM/dMUZMUaaOu0kRu1CLDTRCRgs2s54C53jXla0hd7z/cN4/VZa5FN636Xl+UjuHxzP537OPan3mc/tvb1JzjknAACMJI/1AgAAZxbCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMOU5PG+//bZKS0s1Y8YMJSUl6ZVXXvnBY7Zv364rrrhCPp9P559/vp599tkRLBUAMBF4Dk9vb6/mzJmjhoaGk5q/f/9+XX/99brmmmvU3t6ue++9V7feeqtef/11z4sFAIx/SafyR0KTkpK0detWLVq0aNg5y5Yt07Zt2/Thhx/Gx37zm9/o0KFDam5uHumpAQDj1KREn6C1tVXBYHDQWElJie69995hj+nr61NfX1/837FYTF9++aV+9KMfKSkpKVFLBQB8j3NOhw8f1owZM5ScPDpvC0h4eMLhsPx+/6Axv9+vaDSqr776SlOmTDnumLq6Oq1evTrRSwMAnKSuri795Cc/GZX7Snh4RqK6ulqhUCj+70gkonPPPVddXV1KT08fw5UBwJklGo0qEAho6tSpo3afCQ9Pdna2uru7B411d3crPT19yKsdSfL5fPL5fMeNp6enEx4AGAOj+WuOhH+Op7i4WC0tLYPG3njjDRUXFyf61ACA05Dn8Pzvf/9Te3u72tvbJX3zdun29nZ1dnZK+uZlsvLy8vj8O+64Qx0dHbrvvvu0Z88ebdy4US+++KKWLl06Oo8AADCueA7P+++/r7lz52ru3LmSpFAopLlz56qmpkaS9MUXX8QjJEk//elPtW3bNr3xxhuaM2eOHnvsMT311FMqKSkZpYcAABhPTulzPFai0agyMjIUiUT4HQ8AGErE8y9/qw0AYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEyNKDwNDQ3Ky8tTWlqaioqKtGPHjhPOr6+v14UXXqgpU6YoEAho6dKl+vrrr0e0YADA+OY5PFu2bFEoFFJtba127typOXPmqKSkRAcOHBhy/gsvvKDly5ertrZWu3fv1tNPP60tW7bo/vvvP+XFAwDGH8/hWb9+vW677TZVVlbqkksu0aZNm3TWWWfpmWeeGXL+e++9pwULFmjx4sXKy8vTtddeq5tuuukHr5IAABOTp/D09/erra1NwWDwuztITlYwGFRra+uQx8yfP19tbW3x0HR0dKipqUnXXXfdsOfp6+tTNBoddAMATAyTvEzu6enRwMCA/H7/oHG/3689e/YMeczixYvV09Ojq666Ss45HTt2THfccccJX2qrq6vT6tWrvSwNADBOJPxdbdu3b9fatWu1ceNG7dy5Uy+//LK2bdumNWvWDHtMdXW1IpFI/NbV1ZXoZQIAjHi64snMzFRKSoq6u7sHjXd3dys7O3vIY1atWqUlS5bo1ltvlSRddtll6u3t1e23364VK1YoOfn49vl8Pvl8Pi9LAwCME56ueFJTU1VQUKCWlpb4WCwWU0tLi4qLi4c85siRI8fFJSUlRZLknPO6XgDAOOfpikeSQqGQKioqVFhYqHnz5qm+vl69vb2qrKyUJJWXlys3N1d1dXWSpNLSUq1fv15z585VUVGR9u3bp1WrVqm0tDQeIADAmcNzeMrKynTw4EHV1NQoHA4rPz9fzc3N8TccdHZ2DrrCWblypZKSkrRy5Up9/vnn+vGPf6zS0lI9/PDDo/coAADjRpIbB693RaNRZWRkKBKJKD09fayXAwBnjEQ8//K32gAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwNSIwtPQ0KC8vDylpaWpqKhIO3bsOOH8Q4cOqaqqSjk5OfL5fLrgggvU1NQ0ogUDAMa3SV4P2LJli0KhkDZt2qSioiLV19erpKREe/fuVVZW1nHz+/v79ctf/lJZWVl66aWXlJubq88++0zTpk0bjfUDAMaZJOec83JAUVGRrrzySm3YsEGSFIvFFAgEdPfdd2v58uXHzd+0aZP+/Oc/a8+ePZo8efKIFhmNRpWRkaFIJKL09PQR3QcAwLtEPP96eqmtv79fbW1tCgaD391BcrKCwaBaW1uHPObVV19VcXGxqqqq5Pf7NXv2bK1du1YDAwPDnqevr0/RaHTQDQAwMXgKT09PjwYGBuT3+weN+/1+hcPhIY/p6OjQSy+9pIGBATU1NWnVqlV67LHH9NBDDw17nrq6OmVkZMRvgUDAyzIBAKexhL+rLRaLKSsrS08++aQKCgpUVlamFStWaNOmTcMeU11drUgkEr91dXUlepkAACOe3lyQmZmplJQUdXd3Dxrv7u5Wdnb2kMfk5ORo8uTJSklJiY9dfPHFCofD6u/vV2pq6nHH+Hw++Xw+L0sDAIwTnq54UlNTVVBQoJaWlvhYLBZTS0uLiouLhzxmwYIF2rdvn2KxWHzs448/Vk5OzpDRAQBMbJ5faguFQtq8ebOee+457d69W3feead6e3tVWVkpSSovL1d1dXV8/p133qkvv/xS99xzjz7++GNt27ZNa9euVVVV1eg9CgDAuOH5czxlZWU6ePCgampqFA6HlZ+fr+bm5vgbDjo7O5Wc/F3PAoGAXn/9dS1dulSXX365cnNzdc8992jZsmWj9ygAAOOG58/xjAU+xwMAY2PMP8cDAMCpIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAUyMKT0NDg/Ly8pSWlqaioiLt2LHjpI5rbGxUUlKSFi1aNJLTAgAmAM/h2bJli0KhkGpra7Vz507NmTNHJSUlOnDgwAmP+/TTT/WHP/xBCxcuHPFiAQDjn+fwrF+/XrfddpsqKyt1ySWXaNOmTTrrrLP0zDPPDHvMwMCAbr75Zq1evVozZ848pQUDAMY3T+Hp7+9XW1ubgsHgd3eQnKxgMKjW1tZhj3vwwQeVlZWlW2655aTO09fXp2g0OugGAJgYPIWnp6dHAwMD8vv9g8b9fr/C4fCQx7zzzjt6+umntXnz5pM+T11dnTIyMuK3QCDgZZkAgNNYQt/VdvjwYS1ZskSbN29WZmbmSR9XXV2tSCQSv3V1dSVwlQAAS5O8TM7MzFRKSoq6u7sHjXd3dys7O/u4+Z988ok+/fRTlZaWxsdisdg3J540SXv37tWsWbOOO87n88nn83lZGgBgnPB0xZOamqqCggK1tLTEx2KxmFpaWlRcXHzc/IsuukgffPCB2tvb47cbbrhB11xzjdrb23kJDQDOQJ6ueCQpFAqpoqJChYWFmjdvnurr69Xb26vKykpJUnl5uXJzc1VXV6e0tDTNnj170PHTpk2TpOPGAQBnBs/hKSsr08GDB1VTU6NwOKz8/Hw1NzfH33DQ2dmp5GT+IAIAYGhJzjk31ov4IdFoVBkZGYpEIkpPTx/r5QDAGSMRz79cmgAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgKkRhaehoUF5eXlKS0tTUVGRduzYMezczZs3a+HChZo+fbqmT5+uYDB4wvkAgInNc3i2bNmiUCik2tpa7dy5U3PmzFFJSYkOHDgw5Pzt27frpptu0ltvvaXW1lYFAgFde+21+vzzz0958QCA8SfJOee8HFBUVKQrr7xSGzZskCTFYjEFAgHdfffdWr58+Q8ePzAwoOnTp2vDhg0qLy8/qXNGo1FlZGQoEokoPT3dy3IBAKcgEc+/nq54+vv71dbWpmAw+N0dJCcrGAyqtbX1pO7jyJEjOnr0qM4555xh5/T19SkajQ66AQAmBk/h6enp0cDAgPx+/6Bxv9+vcDh8UvexbNkyzZgxY1C8vq+urk4ZGRnxWyAQ8LJMAMBpzPRdbevWrVNjY6O2bt2qtLS0YedVV1crEonEb11dXYarBAAk0iQvkzMzM5WSkqLu7u5B493d3crOzj7hsY8++qjWrVunN998U5dffvkJ5/p8Pvl8Pi9LAwCME56ueFJTU1VQUKCWlpb4WCwWU0tLi4qLi4c97pFHHtGaNWvU3NyswsLCka8WADDuebrikaRQKKSKigoVFhZq3rx5qq+vV29vryorKyVJ5eXlys3NVV1dnSTpT3/6k2pqavTCCy8oLy8v/rugs88+W2efffYoPhQAwHjgOTxlZWU6ePCgampqFA6HlZ+fr+bm5vgbDjo7O5Wc/N2F1BNPPKH+/n79+te/HnQ/tbW1euCBB05t9QCAccfz53jGAp/jAYCxMeaf4wEA4FQRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAICpEYWnoaFBeXl5SktLU1FRkXbs2HHC+X/729900UUXKS0tTZdddpmamppGtFgAwPjnOTxbtmxRKBRSbW2tdu7cqTlz5qikpEQHDhwYcv57772nm266Sbfccot27dqlRYsWadGiRfrwww9PefEAgPEnyTnnvBxQVFSkK6+8Uhs2bJAkxWIxBQIB3X333Vq+fPlx88vKytTb26vXXnstPvbzn/9c+fn52rRp00mdMxqNKiMjQ5FIROnp6V6WCwA4BYl4/p3kZXJ/f7/a2tpUXV0dH0tOTlYwGFRra+uQx7S2tioUCg0aKykp0SuvvDLsefr6+tTX1xf/dyQSkfTNBgAA7Hz7vOvxGuWEPIWnp6dHAwMD8vv9g8b9fr/27Nkz5DHhcHjI+eFweNjz1NXVafXq1ceNBwIBL8sFAIyS//znP8rIyBiV+/IUHivV1dWDrpIOHTqk8847T52dnaP2wCeCaDSqQCCgrq4uXoL8HvZmaOzL8NiboUUiEZ177rk655xzRu0+PYUnMzNTKSkp6u7uHjTe3d2t7OzsIY/Jzs72NF+SfD6ffD7fceMZGRn8QAwhPT2dfRkGezM09mV47M3QkpNH79M3nu4pNTVVBQUFamlpiY/FYjG1tLSouLh4yGOKi4sHzZekN954Y9j5AICJzfNLbaFQSBUVFSosLNS8efNUX1+v3t5eVVZWSpLKy8uVm5ururo6SdI999yjq6++Wo899piuv/56NTY26v3339eTTz45uo8EADAueA5PWVmZDh48qJqaGoXDYeXn56u5uTn+BoLOzs5Bl2Tz58/XCy+8oJUrV+r+++/Xz372M73yyiuaPXv2SZ/T5/OptrZ2yJffzmTsy/DYm6GxL8Njb4aWiH3x/DkeAABOBX+rDQBgivAAAEwRHgCAKcIDADB12oSHr1oYmpd92bx5sxYuXKjp06dr+vTpCgaDP7iP45nXn5lvNTY2KikpSYsWLUrsAseI1305dOiQqqqqlJOTI5/PpwsuuGBC/v/kdV/q6+t14YUXasqUKQoEAlq6dKm+/vpro9Xaefvtt1VaWqoZM2YoKSnphH9H81vbt2/XFVdcIZ/Pp/PPP1/PPvust5O600BjY6NLTU11zzzzjPvXv/7lbrvtNjdt2jTX3d095Px3333XpaSkuEceecR99NFHbuXKlW7y5Mnugw8+MF55Ynndl8WLF7uGhga3a9cut3v3bvfb3/7WZWRkuH//+9/GK088r3vzrf3797vc3Fy3cOFC96tf/cpmsYa87ktfX58rLCx01113nXvnnXfc/v373fbt2117e7vxyhPL6748//zzzufzueeff97t37/fvf766y4nJ8ctXbrUeOWJ19TU5FasWOFefvllJ8lt3br1hPM7OjrcWWed5UKhkPvoo4/c448/7lJSUlxzc/NJn/O0CM+8efNcVVVV/N8DAwNuxowZrq6ubsj5N954o7v++usHjRUVFbnf/e53CV2nNa/78n3Hjh1zU6dOdc8991yiljhmRrI3x44dc/Pnz3dPPfWUq6iomJDh8bovTzzxhJs5c6br7++3WuKY8LovVVVV7he/+MWgsVAo5BYsWJDQdY61kwnPfffd5y699NJBY2VlZa6kpOSkzzPmL7V9+1ULwWAwPnYyX7Xw/+dL33zVwnDzx6OR7Mv3HTlyREePHh3VP+53Ohjp3jz44IPKysrSLbfcYrFMcyPZl1dffVXFxcWqqqqS3+/X7NmztXbtWg0MDFgtO+FGsi/z589XW1tb/OW4jo4ONTU16brrrjNZ8+lsNJ5/x/yvU1t91cJ4M5J9+b5ly5ZpxowZx/2QjHcj2Zt33nlHTz/9tNrb2w1WODZGsi8dHR36xz/+oZtvvllNTU3at2+f7rrrLh09elS1tbUWy064kezL4sWL1dPTo6uuukrOOR07dkx33HGH7r//fosln9aGe/6NRqP66quvNGXKlB+8jzG/4kFirFu3To2Njdq6davS0tLGejlj6vDhw1qyZIk2b96szMzMsV7OaSUWiykrK0tPPvmkCgoKVFZWphUrVpz0twNPVNu3b9fatWu1ceNG7dy5Uy+//LK2bdumNWvWjPXSJoQxv+Kx+qqF8WYk+/KtRx99VOvWrdObb76pyy+/PJHLHBNe9+aTTz7Rp59+qtLS0vhYLBaTJE2aNEl79+7VrFmzErtoAyP5mcnJydHkyZOVkpISH7v44osVDofV39+v1NTUhK7Zwkj2ZdWqVVqyZIluvfVWSdJll12m3t5e3X777VqxYsWofkXAeDPc8296evpJXe1Ip8EVD1+1MLSR7IskPfLII1qzZo2am5tVWFhosVRzXvfmoosu0gcffKD29vb47YYbbtA111yj9vb2CfPNtiP5mVmwYIH27dsXD7Ekffzxx8rJyZkQ0ZFGti9Hjhw5Li7fxtmd4X/eclSef72/72H0NTY2Op/P55599ln30Ucfudtvv91NmzbNhcNh55xzS5YsccuXL4/Pf/fdd92kSZPco48+6nbv3u1qa2sn7NupvezLunXrXGpqqnvppZfcF198Eb8dPnx4rB5Cwnjdm++bqO9q87ovnZ2dburUqe73v/+927t3r3vttddcVlaWe+ihh8bqISSE132pra11U6dOdX/9619dR0eH+/vf/+5mzZrlbrzxxrF6CAlz+PBht2vXLrdr1y4nya1fv97t2rXLffbZZ84555YvX+6WLFkSn//t26n/+Mc/ut27d7uGhobx+XZq55x7/PHH3bnnnutSU1PdvHnz3D//+c/4f7v66qtdRUXFoPkvvviiu+CCC1xqaqq79NJL3bZt24xXbMPLvpx33nlO0nG32tpa+4Ub8Poz8/9N1PA4531f3nvvPVdUVOR8Pp+bOXOme/jhh92xY8eMV514Xvbl6NGj7oEHHnCzZs1yaWlpLhAIuLvuusv997//tV94gr311ltDPm98ux8VFRXu6quvPu6Y/Px8l5qa6mbOnOn+8pe/eDonX4sAADA15r/jAQCcWQgPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU/8HQWC9leoERecAAAAASUVORK5CYII="},"metadata":{}}]}]}